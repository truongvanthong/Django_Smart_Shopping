{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import underthesea\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df1 = pd.read_csv('../datasets/pre_comments.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Link</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>https://www.thegioididong.com/dtdd/iphone-15-p...</td>\n",
       "      <td>['Máy nóng rất tệ ,call cideo như muốn nổ,đang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>https://www.thegioididong.com/dtdd/iphone-15-p...</td>\n",
       "      <td>['Tốt', 'Máy mới mua sài mau nóng máy quá.khôn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>https://www.thegioididong.com/dtdd/iphone-15-p...</td>\n",
       "      <td>['Rất tuyệt vời', 'Ok', 'Rất Tốt', 'Mọi thứ rấ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>https://fptshop.com.vn/dien-thoai/iphone-15-pr...</td>\n",
       "      <td>['Mình mới mua trả góp. thủ tục nhanh mà nhân ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>https://fptshop.com.vn/dien-thoai/iphone-15-pr...</td>\n",
       "      <td>['Thủ tục mua hàng nhanh, đơn giản nhé', 'Sử d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>iPhone 14 Plus</td>\n",
       "      <td>https://fptshop.com.vn/dien-thoai/iphone-14-pl...</td>\n",
       "      <td>['màu xanh đẹp lắm luôn, cầm chắc tay, màn hìn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>iPhone 14 Plus</td>\n",
       "      <td>https://fptshop.com.vn/dien-thoai/iphone-14-pl...</td>\n",
       "      <td>['máy dùng khá tốt, không có gì để chê, pin cũ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Samsung Galaxy A34 5G</td>\n",
       "      <td>https://www.thegioididong.com/dtdd/samsung-gal...</td>\n",
       "      <td>['Dùng tạm được.nhưng khi mua thì k đc kèm sạc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Samsung Galaxy A34 5G</td>\n",
       "      <td>https://www.thegioididong.com/dtdd/samsung-gal...</td>\n",
       "      <td>['Mình sài gần năm r kh sao mà gần tuần nay cứ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Samsung Galaxy A34 5G</td>\n",
       "      <td>https://fptshop.com.vn/dien-thoai/samsung-gala...</td>\n",
       "      <td>['Ko có gì để chê, nhưng phải trải nghiệm một ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Name                                               Link  \\\n",
       "0        iPhone 15 Pro Max  https://www.thegioididong.com/dtdd/iphone-15-p...   \n",
       "1        iPhone 15 Pro Max  https://www.thegioididong.com/dtdd/iphone-15-p...   \n",
       "2        iPhone 15 Pro Max  https://www.thegioididong.com/dtdd/iphone-15-p...   \n",
       "3        iPhone 15 Pro Max  https://fptshop.com.vn/dien-thoai/iphone-15-pr...   \n",
       "4        iPhone 15 Pro Max  https://fptshop.com.vn/dien-thoai/iphone-15-pr...   \n",
       "..                     ...                                                ...   \n",
       "142         iPhone 14 Plus  https://fptshop.com.vn/dien-thoai/iphone-14-pl...   \n",
       "143         iPhone 14 Plus  https://fptshop.com.vn/dien-thoai/iphone-14-pl...   \n",
       "144  Samsung Galaxy A34 5G  https://www.thegioididong.com/dtdd/samsung-gal...   \n",
       "145  Samsung Galaxy A34 5G  https://www.thegioididong.com/dtdd/samsung-gal...   \n",
       "146  Samsung Galaxy A34 5G  https://fptshop.com.vn/dien-thoai/samsung-gala...   \n",
       "\n",
       "                                              Comments  \n",
       "0    ['Máy nóng rất tệ ,call cideo như muốn nổ,đang...  \n",
       "1    ['Tốt', 'Máy mới mua sài mau nóng máy quá.khôn...  \n",
       "2    ['Rất tuyệt vời', 'Ok', 'Rất Tốt', 'Mọi thứ rấ...  \n",
       "3    ['Mình mới mua trả góp. thủ tục nhanh mà nhân ...  \n",
       "4    ['Thủ tục mua hàng nhanh, đơn giản nhé', 'Sử d...  \n",
       "..                                                 ...  \n",
       "142  ['màu xanh đẹp lắm luôn, cầm chắc tay, màn hìn...  \n",
       "143  ['máy dùng khá tốt, không có gì để chê, pin cũ...  \n",
       "144  ['Dùng tạm được.nhưng khi mua thì k đc kèm sạc...  \n",
       "145  ['Mình sài gần năm r kh sao mà gần tuần nay cứ...  \n",
       "146  ['Ko có gì để chê, nhưng phải trải nghiệm một ...  \n",
       "\n",
       "[147 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df1['Link'] = pre_df1['Link'].str.replace(r'^https://www\\.thegioididong\\.com/.*', 'TGDD', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df1.loc[pre_df1['Link'] != 'TGDD', 'Link'] = 'FPT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Link</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>TGDD</td>\n",
       "      <td>['Máy nóng rất tệ ,call cideo như muốn nổ,đang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>TGDD</td>\n",
       "      <td>['Tốt', 'Máy mới mua sài mau nóng máy quá.khôn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>TGDD</td>\n",
       "      <td>['Rất tuyệt vời', 'Ok', 'Rất Tốt', 'Mọi thứ rấ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>FPT</td>\n",
       "      <td>['Mình mới mua trả góp. thủ tục nhanh mà nhân ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>FPT</td>\n",
       "      <td>['Thủ tục mua hàng nhanh, đơn giản nhé', 'Sử d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>iPhone 14 Plus</td>\n",
       "      <td>FPT</td>\n",
       "      <td>['màu xanh đẹp lắm luôn, cầm chắc tay, màn hìn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>iPhone 14 Plus</td>\n",
       "      <td>FPT</td>\n",
       "      <td>['máy dùng khá tốt, không có gì để chê, pin cũ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Samsung Galaxy A34 5G</td>\n",
       "      <td>TGDD</td>\n",
       "      <td>['Dùng tạm được.nhưng khi mua thì k đc kèm sạc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Samsung Galaxy A34 5G</td>\n",
       "      <td>TGDD</td>\n",
       "      <td>['Mình sài gần năm r kh sao mà gần tuần nay cứ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Samsung Galaxy A34 5G</td>\n",
       "      <td>FPT</td>\n",
       "      <td>['Ko có gì để chê, nhưng phải trải nghiệm một ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Name  Link  \\\n",
       "0        iPhone 15 Pro Max  TGDD   \n",
       "1        iPhone 15 Pro Max  TGDD   \n",
       "2        iPhone 15 Pro Max  TGDD   \n",
       "3        iPhone 15 Pro Max   FPT   \n",
       "4        iPhone 15 Pro Max   FPT   \n",
       "..                     ...   ...   \n",
       "142         iPhone 14 Plus   FPT   \n",
       "143         iPhone 14 Plus   FPT   \n",
       "144  Samsung Galaxy A34 5G  TGDD   \n",
       "145  Samsung Galaxy A34 5G  TGDD   \n",
       "146  Samsung Galaxy A34 5G   FPT   \n",
       "\n",
       "                                              Comments  \n",
       "0    ['Máy nóng rất tệ ,call cideo như muốn nổ,đang...  \n",
       "1    ['Tốt', 'Máy mới mua sài mau nóng máy quá.khôn...  \n",
       "2    ['Rất tuyệt vời', 'Ok', 'Rất Tốt', 'Mọi thứ rấ...  \n",
       "3    ['Mình mới mua trả góp. thủ tục nhanh mà nhân ...  \n",
       "4    ['Thủ tục mua hàng nhanh, đơn giản nhé', 'Sử d...  \n",
       "..                                                 ...  \n",
       "142  ['màu xanh đẹp lắm luôn, cầm chắc tay, màn hìn...  \n",
       "143  ['máy dùng khá tốt, không có gì để chê, pin cũ...  \n",
       "144  ['Dùng tạm được.nhưng khi mua thì k đc kèm sạc...  \n",
       "145  ['Mình sài gần năm r kh sao mà gần tuần nay cứ...  \n",
       "146  ['Ko có gì để chê, nhưng phải trải nghiệm một ...  \n",
       "\n",
       "[147 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(df, name):\n",
    "    comments = df.loc[df['Name'] == name, 'Comments'].values\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name = get_comments(pre_df1, 'iPhone 15 Pro Max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(df, name):\n",
    "    comments = df.loc[df['Name'] == name, 'Comments'].values\n",
    "    return comments\n",
    "def list_comments(l_comments):\n",
    "    all_comments = []\n",
    "    for i in range(len(l_comments)):\n",
    "        split_comments = ast.literal_eval(l_comments[i])\n",
    "        for comment in split_comments:\n",
    "            all_comments.append(comment)\n",
    "    return all_comments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_com = list_comments(product_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "class SVCModel():\n",
    "    def __init__(self, kernel='rbf', C=1.0, gamma='scale', attribute=None):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        self.gamma = gamma\n",
    "        self.svc = None\n",
    "        self.attribute = attribute\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.svc = SVC(kernel=self.kernel, C=self.C, gamma=self.gamma, probability=True)\n",
    "        self.svc.fit(X, y[self.attribute])\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.svc is None:\n",
    "            raise ValueError(\"SVC model has not been trained yet. Please call 'fit' first.\")\n",
    "        return self.svc.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if self.svc is None:\n",
    "            raise ValueError(\"SVC model has not been trained yet. Please call 'fit' first.\")\n",
    "        return self.svc.predict_proba(X)\n",
    "    \n",
    "    def calculate_accuracy_score(self, X, y, y_pred=None):\n",
    "        if self.svc is None:\n",
    "            raise ValueError(\"SVC model has not been trained yet. Please call 'fit' first.\")\n",
    "        \n",
    "        if y_pred is not None:\n",
    "            accuracy = accuracy_score(y[self.attribute], y_pred)\n",
    "            return accuracy\n",
    "        \n",
    "        y_pred = self.predict(X)\n",
    "        accuracy = accuracy_score(y[self.attribute], y_pred)\n",
    "        return accuracy\n",
    "    \n",
    "    def calculate_f1_score(self, X, y, y_pred=None):\n",
    "        if self.svc is None:\n",
    "            raise ValueError(\"SVC model has not been trained yet. Please call 'fit' first.\")\n",
    "\n",
    "        if y_pred is not None:\n",
    "            if y[self.attribute].nunique() < 3:\n",
    "                f1 = f1_score(y[self.attribute], y_pred)\n",
    "            else:\n",
    "                f1 = f1_score(y[self.attribute], y_pred, average='micro')\n",
    "            return f1\n",
    "\n",
    "        y_pred = self.predict(X)\n",
    "        if y[self.attribute].nunique() < 3:\n",
    "            f1 = f1_score(y[self.attribute], y_pred)\n",
    "        else:\n",
    "            f1 = f1_score(y[self.attribute], y_pred, average='micro')\n",
    "        return f1\n",
    "    \n",
    "    def calculate_precision_score(self, X, y, y_pred=None):\n",
    "        if self.svc is None:\n",
    "            raise ValueError(\"SVC model has not been trained yet. Please call 'fit' first.\")\n",
    "        \n",
    "        if y_pred is not None:\n",
    "            if y[self.attribute].nunique() < 3:\n",
    "                precision = precision_score(y[self.attribute], y_pred)\n",
    "            else:\n",
    "                precision = precision_score(y[self.attribute], y_pred, average='micro')\n",
    "            return precision\n",
    "        \n",
    "        y_pred = self.predict(X)\n",
    "        if y[self.attribute].nunique() < 3:\n",
    "            precision = precision_score(y[self.attribute], y_pred)\n",
    "        else:\n",
    "            precision = precision_score(y[self.attribute], y_pred, average='micro')\n",
    "        return precision\n",
    "    \n",
    "    def calculate_recall_score(self, X, y, y_pred=None):\n",
    "        if self.svc is None:\n",
    "            raise ValueError(\"SVC model has not been trained yet. Please call 'fit' first.\")\n",
    "        \n",
    "        if y_pred is not None:\n",
    "            if y[self.attribute].nunique() < 3:\n",
    "                recall = recall_score(y[self.attribute], y_pred)\n",
    "            else:\n",
    "                recall = recall_score(y[self.attribute], y_pred, average='micro')\n",
    "            return recall\n",
    "        \n",
    "        y_pred = self.predict(X)\n",
    "        if y[self.attribute].nunique() < 3:\n",
    "            recall = recall_score(y[self.attribute], y_pred)\n",
    "        else:\n",
    "            recall = recall_score(y[self.attribute], y_pred, average='micro')\n",
    "        return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess.py\n",
    "import regex as re\n",
    "from underthesea import word_tokenize\n",
    "import string\n",
    "import codecs\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "\n",
    "# remove html tags\n",
    "def remove_html(txt):\n",
    "    return re.sub(r'<[^>]*>', '', txt)\n",
    "\n",
    "# unicode stardard\n",
    "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
    "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
    "\n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "        '|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "dicchar = loaddicchar()\n",
    "\n",
    "def convert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)\n",
    "\n",
    "# diacritic standard\n",
    "vowels_to_ids = {}\n",
    "vowels_table = [\n",
    "    ['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a' ],\n",
    "    ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "    ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "    ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e' ],\n",
    "    ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "    ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i' ],\n",
    "    ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o' ],\n",
    "    ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
    "    ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "    ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u' ],\n",
    "    ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "    ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y' ]\n",
    "]\n",
    "\n",
    "for i in range(len(vowels_table)):\n",
    "    for j in range(len(vowels_table[i]) - 1):\n",
    "        vowels_to_ids[vowels_table[i][j]] = (i, j)\n",
    "\n",
    "def is_valid_vietnamese_word(word):\n",
    "    chars = list(word)\n",
    "    vowel_indexes = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = vowels_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if vowel_indexes == -1: vowel_indexes = index\n",
    "            else:\n",
    "                if index - vowel_indexes != 1: return False\n",
    "                vowel_indexes = index\n",
    "    return True\n",
    "\n",
    "def standardize_word_typing(word):\n",
    "    if not is_valid_vietnamese_word(word): return word\n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    vowel_indexes = []\n",
    "    qu_or_gi = False\n",
    "\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = vowels_to_ids.get(char, (-1, -1))\n",
    "        if x == -1: continue\n",
    "        elif x == 9:  # check qu\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:  # check gi\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = vowels_table[x][0]\n",
    "\n",
    "        if not qu_or_gi or index != 1:\n",
    "            vowel_indexes.append(index)\n",
    "\n",
    "    if len(vowel_indexes) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = vowels_to_ids.get(chars[1])\n",
    "                chars[1] = vowels_table[x][dau_cau]\n",
    "            else:\n",
    "                x, y = vowels_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1: chars[2] = vowels_table[x][dau_cau]\n",
    "                else: chars[1] = vowels_table[5][dau_cau] if chars[1] == 'i' else vowels_table[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    "\n",
    "    for index in vowel_indexes:\n",
    "        x, y = vowels_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # ê, ơ\n",
    "            chars[index] = vowels_table[x][dau_cau]\n",
    "            return ''.join(chars)\n",
    "\n",
    "    if len(vowel_indexes) == 2:\n",
    "        if vowel_indexes[-1] == len(chars) - 1:\n",
    "            x, y = vowels_to_ids[chars[vowel_indexes[0]]]\n",
    "            chars[vowel_indexes[0]] = vowels_table[x][dau_cau]\n",
    "        else:\n",
    "            x, y = vowels_to_ids[chars[vowel_indexes[1]]]\n",
    "            chars[vowel_indexes[1]] = vowels_table[x][dau_cau]\n",
    "    else:\n",
    "        x, y = vowels_to_ids[chars[vowel_indexes[1]]]\n",
    "        chars[vowel_indexes[1]] = vowels_table[x][dau_cau]\n",
    "    return ''.join(chars)\n",
    "\n",
    "def standardize_sentence_typing(text):\n",
    "    words = text.lower().split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
    "        if len(cw) == 3: cw[1] = standardize_word_typing(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)\n",
    "\n",
    "# remove unnecessary characters\n",
    "def remove_unnecessary(text):\n",
    "    text = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđÁÀẢÃẠĂẮẰẲẴẶÂẤẦẨẪẬÉÈẺẼẸÊẾỀỂỄỆÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÍÌỈĨỊÚÙỦŨỤƯỨỪỬỮỰÝỲỶỸỴĐ_]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "# special wordings in reviews (e.g., emoji,...)\n",
    "def normalize_money(sent):\n",
    "    return re.sub(r'[0-9]+[.,0-9][k-m-b]', 'giá', sent)\n",
    "\n",
    "def normalize_hastag(sent):\n",
    "    return re.sub(r'#+\\w+', 'tag', sent)\n",
    "\n",
    "def normalize_website(sent):\n",
    "    result = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'website', sent)\n",
    "    return re.sub(r'\\w+(\\.(com|vn|me))+((\\/+([\\.\\w\\_\\-]+)?)+)?', 'website', result)\n",
    "\n",
    "def nomalize_emoji(sent):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                u\"\\U00002702-\\U000027B0\"\n",
    "                u\"\\U000024C2-\\U0001F251\"\n",
    "                u\"\\U0001f926-\\U0001f937\"\n",
    "                u'\\U00010000-\\U0010ffff'\n",
    "                u\"\\u200d\"\n",
    "                u\"\\u2640-\\u2642\"\n",
    "                u\"\\u2600-\\u2B55\"\n",
    "                u\"\\u23cf\"\n",
    "                u\"\\u23e9\"\n",
    "                u\"\\u231a\"\n",
    "                u\"\\u3030\"\n",
    "                u\"\\ufe0f\"\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', sent)\n",
    "\n",
    "def normalize_elongate(sent):\n",
    "    patern = r'(.)\\1{1,}'\n",
    "    result = sent\n",
    "    while(re.search(patern, result) != None):\n",
    "        repeat_char = re.search(patern, result)\n",
    "        result = result.replace(repeat_char[0], repeat_char[1])\n",
    "    return result\n",
    "\n",
    "def remove_numbers(sent):\n",
    "    return re.sub(r'[0-9]+', '', sent)\n",
    "\n",
    "def normalize_acronyms(sent):\n",
    "    text = sent\n",
    "    replace_list = {\n",
    "            \"òa\": \"oà\",\n",
    "        \"Òa\": \"Oà\",\n",
    "        \"ÒA\": \"OÀ\",\n",
    "        \"óa\": \"oá\",\n",
    "        \"Óa\": \"Oá\",\n",
    "        \"ÓA\": \"OÁ\",\n",
    "        \"ỏa\": \"oả\",\n",
    "        \"Ỏa\": \"Oả\",\n",
    "        \"ỎA\": \"OẢ\",\n",
    "        \"õa\": \"oã\",\n",
    "        \"Õa\": \"Oã\",\n",
    "        \"ÕA\": \"OÃ\",\n",
    "        \"ọa\": \"oạ\",\n",
    "        \"Ọa\": \"Oạ\",\n",
    "        \"ỌA\": \"OẠ\",\n",
    "        \"òe\": \"oè\",\n",
    "        \"Òe\": \"Oè\",\n",
    "        \"ÒE\": \"OÈ\",\n",
    "        \"óe\": \"oé\",\n",
    "        \"Óe\": \"Oé\",\n",
    "        \"ÓE\": \"OÉ\",\n",
    "        \"ỏe\": \"oẻ\",\n",
    "        \"Ỏe\": \"Oẻ\",\n",
    "        \"ỎE\": \"OẺ\",\n",
    "        \"õe\": \"oẽ\",\n",
    "        \"Õe\": \"Oẽ\",\n",
    "        \"ÕE\": \"OẼ\",\n",
    "        \"ọe\": \"oẹ\",\n",
    "        \"Ọe\": \"Oẹ\",\n",
    "        \"ỌE\": \"OẸ\",\n",
    "        \"ùy\": \"uỳ\",\n",
    "        \"Ùy\": \"Uỳ\",\n",
    "        \"ÙY\": \"UỲ\",\n",
    "        \"úy\": \"uý\",\n",
    "        \"Úy\": \"Uý\",\n",
    "        \"ÚY\": \"UÝ\",\n",
    "        \"ủy\": \"uỷ\",\n",
    "        \"Ủy\": \"Uỷ\",\n",
    "        \"ỦY\": \"UỶ\",\n",
    "        \"ũy\": \"uỹ\",\n",
    "        \"Ũy\": \"Uỹ\",\n",
    "        \"ŨY\": \"UỸ\",\n",
    "        \"ụy\": \"uỵ\",\n",
    "        \"Ụy\": \"Uỵ\",\n",
    "        \"ỤY\": \"UỴ\",\n",
    "        'ả': 'ả', 'ố': 'ố', 'u´': 'ố','ỗ': 'ỗ', 'ồ': 'ồ', 'ổ': 'ổ', 'ấ': 'ấ', 'ẫ': 'ẫ', 'ẩ': 'ẩ',\n",
    "        'ầ': 'ầ', 'ỏ': 'ỏ', 'ề': 'ề','ễ': 'ễ', 'ắ': 'ắ', 'ủ': 'ủ', 'ế': 'ế', 'ở': 'ở', 'ỉ': 'ỉ',\n",
    "        'ẻ': 'ẻ', 'àk': u' à ','aˋ': 'à', 'iˋ': 'ì', 'ă´': 'ắ','ử': 'ử', 'e˜': 'ẽ', 'y˜': 'ỹ', 'a´': 'á',\n",
    "            #Quy các icon về 2 loại emoj: Tích cực hoặc tiêu cực\n",
    "            \"👹\": \"Tiêu cực\", \"👻\": \"Tích cực\", \"💃\": \"Tích cực\",'🤙': ' Tích cực ', '👍': ' Tích cực ',\n",
    "            \"💄\": \"Tích cực\", \"💎\": \"Tích cực\", \"💩\": \"Tích cực\",\"😕\": \"Tiêu cực\", \"😱\": \"Tiêu cực\", \"😸\": \"Tích cực\",\n",
    "            \"😾\": \"Tiêu cực\", \"🚫\": \"Tiêu cực\",  \"🤬\": \"Tiêu cực\",\"🧚\": \"Tích cực\", \"🧡\": \"Tích cực\",'🐶':' Tích cực ',\n",
    "            '👎': ' Tiêu cực ', '😣': ' Tiêu cực ','✨': ' Tích cực ', '❣': ' Tích cực ','☀': ' Tích cực ',\n",
    "            '♥': ' Tích cực ', '🤩': ' Tích cực ', 'like': ' Tích cực ', '💌': ' Tích cực ',\n",
    "            '🤣': ' Tích cực ', '🖤': ' Tích cực ', '🤤': ' Tích cực ', ':(': ' Tiêu cực ', '😢': ' Tiêu cực ',\n",
    "            '❤': ' Tích cực ', '😍': ' Tích cực ', '😘': ' Tích cực ', '😪': ' Tiêu cực ', '😊': ' Tích cực ',\n",
    "            '?': ' ? ', '😁': ' Tích cực ', '💖': ' Tích cực ', '😟': ' Tiêu cực ', '😭': ' Tiêu cực ',\n",
    "            '💯': ' Tích cực ', '💗': ' Tích cực ', '♡': ' Tích cực ', '💜': ' Tích cực ', '🤗': ' Tích cực ',\n",
    "            '^^': ' Tích cực ', '😨': ' Tiêu cực ', '☺': ' Tích cực ', '💋': ' Tích cực ', '👌': ' Tích cực ',\n",
    "            '😖': ' Tiêu cực ', '😀': ' Tích cực ', ':((': ' Tiêu cực ', '😡': ' Tiêu cực ', '😠': ' Tiêu cực ',\n",
    "            '😒': ' Tiêu cực ', '🙂': ' Tích cực ', '😏': ' Tiêu cực ', '😝': ' Tích cực ', '😄': ' Tích cực ',\n",
    "            '😙': ' Tích cực ', '😤': ' Tiêu cực ', '😎': ' Tích cực ', '😆': ' Tích cực ', '💚': ' Tích cực ',\n",
    "            '✌': ' Tích cực ', '💕': ' Tích cực ', '😞': ' Tiêu cực ', '😓': ' Tiêu cực ', '️🆗️': ' Tích cực ',\n",
    "            '😉': ' Tích cực ', '😂': ' Tích cực ', ':v': '  Tích cực ', '=))': '  Tích cực ', '😋': ' Tích cực ',\n",
    "            '💓': ' Tích cực ', '😐': ' Tiêu cực ', ':3': ' Tích cực ', '😫': ' Tiêu cực ', '😥': ' Tiêu cực ',\n",
    "            '😃': ' Tích cực ', '😬': ' 😬 ', '😌': ' 😌 ', '💛': ' Tích cực ', '🤝': ' Tích cực ', '🎈': ' Tích cực ',\n",
    "            '😗': ' Tích cực ', '🤔': ' Tiêu cực ', '😑': ' Tiêu cực ', '🔥': ' Tiêu cực ', '🙏': ' Tiêu cực ',\n",
    "            '🆗': ' Tích cực ', '😻': ' Tích cực ', '💙': ' Tích cực ', '💟': ' Tích cực ',\n",
    "            '😚': ' Tích cực ', '❌': ' Tiêu cực ', '👏': ' Tích cực ', ';)': ' Tích cực ', '<3': ' Tích cực ',\n",
    "            '🌝': ' Tích cực ',  '🌷': ' Tích cực ', '🌸': ' Tích cực ', '🌺': ' Tích cực ',\n",
    "            '🌼': ' Tích cực ', '🍓': ' Tích cực ', '🐅': ' Tích cực ', '🐾': ' Tích cực ', '👉': ' Tích cực ',\n",
    "            '💐': ' Tích cực ', '💞': ' Tích cực ', '💥': ' Tích cực ', '💪': ' Tích cực ',\n",
    "            '💰': ' Tích cực ',  '😇': ' Tích cực ', '😛': ' Tích cực ', '😜': ' Tích cực ',\n",
    "            '🙃': ' Tích cực ', '🤑': ' Tích cực ', '🤪': ' Tích cực ','☹': ' Tiêu cực ',  '💀': ' Tiêu cực ',\n",
    "            '😔': ' Tiêu cực ', '😧': ' Tiêu cực ', '😩': ' Tiêu cực ', '😰': ' Tiêu cực ', '😳': ' Tiêu cực ',\n",
    "            '😵': ' Tiêu cực ', '😶': ' Tiêu cực ', '🙁': ' Tiêu cực ',\n",
    "            #Chuẩn hóa 1 số sentiment words/English words\n",
    "            ':))': '  Tích cực ', ':)': ' Tích cực ', 'ô kêi': ' ok ', 'okie': ' ok ', ' o kê ': ' ok ',\n",
    "            'okey': ' ok ', 'ôkê': ' ok ', 'oki': ' ok ', ' oke ':  ' ok ',' okay':' ok ','okê':' ok ',\n",
    "            ' tks ': u' cám ơn ', 'thks': u' cám ơn ', 'thanks': u' cám ơn ', 'ths': u' cám ơn ', 'thank': u' cám ơn ',\n",
    "            '⭐': 'star ', '*': 'star ', '🌟': 'star ', '🎉': u' Tích cực ',\n",
    "            'kg ': u' không ','not': u' không ', u' kg ': u' không ', '\"k ': u' không ',' kh ':u' không ','kô':u' không ','hok':u' không ',' kp ': u' không phải ',u' kô ': u' không ', '\"ko ': u' không ', u' ko ': u' không ', u' k ': u' không ', 'khong': u' không ', u' hok ': u' không ',\n",
    "            'he he': ' Tích cực ','hehe': ' Tích cực ','hihi': ' Tích cực ', 'haha': ' Tích cực ', 'hjhj': ' Tích cực ',\n",
    "            ' lol ': ' Tiêu cực ',' cc ': ' Tiêu cực ','cute': u' dễ thương ','huhu': ' Tiêu cực ', ' vs ': u' với ', 'wa': ' quá ', 'wá': u' quá', 'j': u' gì ', '“': ' ',\n",
    "            ' sz ': u' cỡ ', 'size': u' cỡ ', u' đx ': u' được ', 'dk': u' được  ', 'dc': u' được ', 'đk': u' được ',\n",
    "            'đc': u' được ','authentic': u' chuẩn chính hãng ',u' aut ': u' chuẩn chính hãng ', u' auth ': u' chuẩn chính hãng ', 'thick': u' Tích cực ', 'store': u' cửa hàng ',\n",
    "            'shop': u' cửa hàng ', 'sp': u' sản phẩm ', 'gud': u' tốt ','god': u' tốt ','wel done':' tốt ', 'good': u' tốt ', 'gút': u' tốt ',\n",
    "            'sấu': u' xấu ','gut': u' tốt ', u' tot ': u' tốt ', u' nice ': u' tốt ', 'perfect': 'rất tốt', 'bt': u' bình thường ',\n",
    "            'time': u' thời gian ', 'qá': u' quá ', u' ship ': u' giao hàng ', u' m ': u' mình ', u' mik ': u' mình ',\n",
    "            'ể': 'ể', 'product': 'sản phẩm', 'quality': 'chất lượng','chat':' chất ', 'excelent': 'hoàn hảo', 'bad': 'tệ','fresh': ' tươi ','sad': ' tệ ',\n",
    "            'date': u' hạn sử dụng ', 'hsd': u' hạn sử dụng ','quickly': u' nhanh ', 'quick': u' nhanh ','fast': u' nhanh ','delivery': u' giao hàng ',u' síp ': u' giao hàng ',\n",
    "            'beautiful': u' đẹp tuyệt vời ', u' tl ': u' trả lời ', u' r ': u' rồi ', u' shopE ': u' cửa hàng ',u' order ': u' đặt hàng ',\n",
    "            'chất lg': u' chất lượng ',u' sd ': u' sử dụng ',u' dt ': u' điện thoại ',u' nt ': u' nhắn tin ',u' tl ': u' trả lời ',u' sài ': u' xài ',u'bjo':u' bao giờ ',\n",
    "            'thik': u' thích ',u' sop ': u' cửa hàng ', ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': u' rất ',u'quả ng ':u' quảng  ',\n",
    "            'dep': u' đẹp ',u' xau ': u' xấu ','delicious': u' ngon ', u'hàg': u' hàng ', u'qủa': u' quả ',\n",
    "            'iu': u' yêu ','fake': u' giả mạo ', 'trl': 'trả lời', '><': u' Tích cực ', 'nv' : 'nhân viên', 'nvien' : 'nhân viên',\n",
    "            ' por ': u' tệ ',' poor ': u' tệ ', 'ib':u' nhắn tin ', 'rep':u' trả lời ',u'fback':' feedback ','fedback':' feedback ', 'pùn' : 'buồn', 'tuỵt vời' : 'tuyệt vời',\n",
    "            #dưới 3* quy về 1*, trên 3* quy về 5*\n",
    "            '6 sao': ' 5star ','6 star': ' 5star ', '5star': ' 5star ','5 sao': ' 5star ','5sao': ' 5star ',\n",
    "            'starstarstarstarstar': ' 5star ', '1 sao': ' 1star ', '1sao': ' 1star ','2 sao':' 1star ','2sao':' 1star ',\n",
    "            '2 starstar':' 1star ','1star': ' 1star ', '0 sao': ' 1star ', '0star': ' 1star ',\n",
    "            }\n",
    "\n",
    "    for k, v in replace_list.items():\n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "\n",
    "# Từ điển tích cực, tiêu cực, phủ định\n",
    "def load_sentiment_dicts(path_pos, path_nag, path_not):\n",
    "    with codecs.open(path_pos, 'r', encoding='UTF-8') as f:\n",
    "        pos = f.readlines()\n",
    "    pos_list = [n.strip() for n in pos]\n",
    "\n",
    "    with codecs.open(path_nag, 'r', encoding='UTF-8') as f:\n",
    "        nag = f.readlines()\n",
    "    nag_list = [n.strip() for n in nag]\n",
    "\n",
    "    with codecs.open(path_not, 'r', encoding='UTF-8') as f:\n",
    "        not_ = f.readlines()\n",
    "    not_list = [n.strip() for n in not_]\n",
    "\n",
    "    return pos_list, nag_list, not_list\n",
    "\n",
    "# Phân tích tình cảm bằng từ điển được xác định trước\n",
    "def add_sentiment_features(text, pos_list, nag_list, not_list):\n",
    "    # chuyen punctuation thành space\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    text = ViTokenizer.tokenize(text)\n",
    "    texts = text.split()\n",
    "    len_text = len(texts)\n",
    "\n",
    "    texts = [t.replace('_', ' ') for t in texts]\n",
    "    for i in range(len_text):\n",
    "        cp_text = texts[i]\n",
    "        if cp_text in not_list: # Xử lý vấn đề phủ định (VD: áo này chẳng đẹp--> áo này notpos)\n",
    "            numb_word = 2 if len_text - i - 1 >= 4 else len_text - i - 1\n",
    "\n",
    "            for j in range(numb_word):\n",
    "                if texts[i + j + 1] in pos_list:\n",
    "                    texts[i] = 'notpos'\n",
    "                    texts[i + j + 1] = ''\n",
    "\n",
    "                if texts[i + j + 1] in nag_list:\n",
    "                    texts[i] = 'notnag'\n",
    "                    texts[i + j + 1] = ''\n",
    "        else: #Thêm feature cho những sentiment words (áo này đẹp--> áo này đẹp Tích cực)\n",
    "            if cp_text in pos_list:\n",
    "                texts.append('Tích cực')\n",
    "            elif cp_text in nag_list:\n",
    "                texts.append('Tiêu cực')\n",
    "\n",
    "    text = u' '.join(texts)\n",
    "\n",
    "    #remove nốt những ký tự thừa thãi\n",
    "    text = text.replace(u'\"', u' ')\n",
    "    text = text.replace(u'️', u'')\n",
    "    text = text.replace('🏻','')\n",
    "    return text\n",
    "\n",
    "# overall preprocessing\n",
    "def text_preprocess(document, pos_list, nag_list, not_list):\n",
    "    #đưa về lower\n",
    "    document = document.lower()\n",
    "    # xóa html code\n",
    "    document = remove_html(document)\n",
    "    # chuẩn hóa unicode\n",
    "    document = convert_unicode(document)\n",
    "    \n",
    "    # chuẩn hóa các ký tự đặc biệt\n",
    "    document = normalize_money(document)\n",
    "    document = normalize_hastag(document)\n",
    "    document = normalize_website(document)\n",
    "    document = nomalize_emoji(document)\n",
    "    document = normalize_elongate(document)\n",
    "    document = normalize_acronyms(document)\n",
    "    document = remove_numbers(document)\n",
    "\n",
    "    # chuẩn hóa cách gõ dấu tiếng việt\n",
    "    document = standardize_sentence_typing(document)\n",
    "    # tách từ\n",
    "    document = word_tokenize(document, format=\"text\")\n",
    "    # đưa về lower\n",
    "    document = document.lower()\n",
    "    # xóa các ký tự không cần thiết\n",
    "    document = remove_unnecessary(document)\n",
    "    #\n",
    "    document = add_sentiment_features(document, pos_list, nag_list, not_list)\n",
    "    return document.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation))).replace(' '*4, ' ').replace(' '*3, ' ').replace(' '*2, ' ').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pos = '../sentiment_dicts/not.txt'\n",
    "path_nag = '../sentiment_dicts/nag.txt'\n",
    "path_not = '../sentiment_dicts/not.txt'\n",
    "pos_list, nag_list, not_list = load_sentiment_dicts(path_pos, path_nag, path_not)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer.py\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from underthesea import word_tokenize\n",
    "def tokenize_all_reviews(embed_model, dims, reviews, pos_list, nag_list, not_list):\n",
    "    # split each review into a list of words\n",
    "    # reviews = [text_preprocess(review, pos_list, nag_list, not_list) for review in reviews]\n",
    "    # reviews_words = [ word_tokenize(review, format='text') for review in reviews]\n",
    "    reviews_words = [text_preprocess(review, pos_list, nag_list, not_list) for review in reviews]\n",
    "    # reviews_words = [word_tokenize(review) for review in reviews_words]\n",
    "    reviews_words = [reviews_word.split() for reviews_word in reviews_words]\n",
    "\n",
    "    tokenized_reviews = []\n",
    "    for review in reviews_words:\n",
    "        mean_embedding = np.zeros(dims)\n",
    "        num_words = 0\n",
    "        for word in review:\n",
    "            try:\n",
    "                mean_embedding += embed_model[word]\n",
    "                num_words += 1\n",
    "            except:\n",
    "                continue\n",
    "        mean_embedding /= num_words\n",
    "        tokenized_reviews.append(mean_embedding)\n",
    "\n",
    "    return tokenized_reviews \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load('/Mobile-e-commerce-review-sentiment-classification-main/w2v/w2v_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Link</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>TGDD</td>\n",
       "      <td>['Máy nóng rất tệ ,call cideo như muốn nổ,đang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>TGDD</td>\n",
       "      <td>['Tốt', 'Máy mới mua sài mau nóng máy quá.khôn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>TGDD</td>\n",
       "      <td>['Rất tuyệt vời', 'Ok', 'Rất Tốt', 'Mọi thứ rấ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>FPT</td>\n",
       "      <td>['Mình mới mua trả góp. thủ tục nhanh mà nhân ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>FPT</td>\n",
       "      <td>['Thủ tục mua hàng nhanh, đơn giản nhé', 'Sử d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>iPhone 14 Plus</td>\n",
       "      <td>FPT</td>\n",
       "      <td>['màu xanh đẹp lắm luôn, cầm chắc tay, màn hìn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>iPhone 14 Plus</td>\n",
       "      <td>FPT</td>\n",
       "      <td>['máy dùng khá tốt, không có gì để chê, pin cũ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Samsung Galaxy A34 5G</td>\n",
       "      <td>TGDD</td>\n",
       "      <td>['Dùng tạm được.nhưng khi mua thì k đc kèm sạc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Samsung Galaxy A34 5G</td>\n",
       "      <td>TGDD</td>\n",
       "      <td>['Mình sài gần năm r kh sao mà gần tuần nay cứ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Samsung Galaxy A34 5G</td>\n",
       "      <td>FPT</td>\n",
       "      <td>['Ko có gì để chê, nhưng phải trải nghiệm một ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Name  Link  \\\n",
       "0        iPhone 15 Pro Max  TGDD   \n",
       "1        iPhone 15 Pro Max  TGDD   \n",
       "2        iPhone 15 Pro Max  TGDD   \n",
       "3        iPhone 15 Pro Max   FPT   \n",
       "4        iPhone 15 Pro Max   FPT   \n",
       "..                     ...   ...   \n",
       "142         iPhone 14 Plus   FPT   \n",
       "143         iPhone 14 Plus   FPT   \n",
       "144  Samsung Galaxy A34 5G  TGDD   \n",
       "145  Samsung Galaxy A34 5G  TGDD   \n",
       "146  Samsung Galaxy A34 5G   FPT   \n",
       "\n",
       "                                              Comments  \n",
       "0    ['Máy nóng rất tệ ,call cideo như muốn nổ,đang...  \n",
       "1    ['Tốt', 'Máy mới mua sài mau nóng máy quá.khôn...  \n",
       "2    ['Rất tuyệt vời', 'Ok', 'Rất Tốt', 'Mọi thứ rấ...  \n",
       "3    ['Mình mới mua trả góp. thủ tục nhanh mà nhân ...  \n",
       "4    ['Thủ tục mua hàng nhanh, đơn giản nhé', 'Sử d...  \n",
       "..                                                 ...  \n",
       "142  ['màu xanh đẹp lắm luôn, cầm chắc tay, màn hìn...  \n",
       "143  ['máy dùng khá tốt, không có gì để chê, pin cũ...  \n",
       "144  ['Dùng tạm được.nhưng khi mua thì k đc kèm sạc...  \n",
       "145  ['Mình sài gần năm r kh sao mà gần tuần nay cứ...  \n",
       "146  ['Ko có gì để chê, nhưng phải trải nghiệm một ...  \n",
       "\n",
       "[147 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(df, name, company):\n",
    "    comments = df.loc[(df['Name'] == name) & (df['Link'] == company), 'Comments'].values\n",
    "    return comments\n",
    "def list_comments(l_comments):\n",
    "    all_comments = []\n",
    "    for i in range(len(l_comments)):\n",
    "        split_comments = ast.literal_eval(l_comments[i])\n",
    "        for comment in split_comments:\n",
    "            all_comments.append(comment)\n",
    "    return all_comments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name = get_comments(pre_df1, 'Samsung Galaxy A34 5G', 'FPT')\n",
    "product_name = list_comments(product_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_word = tokenize_all_reviews(w2v, 300, product_name, pos_list, nag_list, not_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_word_np = np.array(vec_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 300)"
      ]
     },
     "execution_count": 808,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_word_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_mask = np.isnan(vec_word_np)\n",
    "\n",
    "rows_with_nan = nan_mask.any(axis=1)\n",
    "\n",
    "cleaned_array = vec_word_np[~rows_with_nan]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(342, 300)"
      ]
     },
     "execution_count": 799,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_word = cleaned_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "345"
      ]
     },
     "execution_count": 798,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(product_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trnmah/mambaforge/envs/practic1/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.2.2 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "# svm_Pin = joblib.load('/home/trnmah/CNPM66/Mobile-e-commerce-review-sentiment-classification-main/pretrained_svm/bestModelPin')\n",
    "# svm_Service = joblib.load('/home/trnmah/CNPM66/Mobile-e-commerce-review-sentiment-classification-main/pretrained_svm/bestModelService')\n",
    "# svm_General = joblib.load('/home/trnmah/CNPM66/Mobile-e-commerce-review-sentiment-classification-main/pretrained_svm/bestModelGeneral')\n",
    "# svm_Others = joblib.load('/home/trnmah/CNPM66/Mobile-e-commerce-review-sentiment-classification-main/pretrained_svm/bestModelOthers')\n",
    "\n",
    "svm_SPin = joblib.load('/home/trnmah/CNPM66/Mobile-e-commerce-review-sentiment-classification-main/pretrained_svm/bestModelSPin')\n",
    "svm_SSer = joblib.load('/home/trnmah/CNPM66/Mobile-e-commerce-review-sentiment-classification-main/pretrained_svm/bestModelSSer')\n",
    "svm_SGeneral = joblib.load('/home/trnmah/CNPM66/Mobile-e-commerce-review-sentiment-classification-main/pretrained_svm/bestModelSGeneral')\n",
    "svm_SOth = joblib.load('/home/trnmah/CNPM66/Mobile-e-commerce-review-sentiment-classification-main/pretrained_svm/bestModelSOth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_Pin = svm_Pin.predict(vec_word_np)\n",
    "# y_pred_Service = svm_Service.predict(vec_word_np)\n",
    "# y_pred_General = svm_General.predict(vec_word_np)\n",
    "# y_pred_Others = svm_Others.predict(vec_word_np)\n",
    "\n",
    "# y_pred_SPin = svm_SPin.predict_proba(vec_word)\n",
    "# y_pred_SSer = svm_SSer.predict_proba(vec_word)\n",
    "# y_pred_SGeneral = svm_SGeneral.predict_proba(vec_word)\n",
    "# y_pred_SOth = svm_SOth.predict_proba(vec_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_SPin = svm_SPin.predict_proba(vec_word)\n",
    "y_pred_SSer = svm_SSer.predict_proba(vec_word)\n",
    "y_pred_SGeneral = svm_SGeneral.predict_proba(vec_word)\n",
    "y_pred_SOth = svm_SOth.predict_proba(vec_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04756708 0.7865609  0.16587201]\n",
      "[0.0428561  0.50355833 0.45358557]\n",
      "[0.04055891 0.54013136 0.41930973]\n",
      "[0.07349256 0.30205295 0.62445449]\n"
     ]
    }
   ],
   "source": [
    "# print(y_pred_Pin.mean(axis=0))\n",
    "# print(y_pred_Service.mean(axis=0))\n",
    "# print(y_pred_General.mean(axis=0))\n",
    "# print(y_pred_Others.mean(axis=0))\n",
    "print(y_pred_SPin.mean(axis=0))\n",
    "print(y_pred_SSer.mean(axis=0))\n",
    "print(y_pred_SGeneral.mean(axis=0))\n",
    "print(y_pred_SOth.mean(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practic1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
