{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import underthesea\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df1 = pd.read_csv('../datasets/pre_comments.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Link</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>https://www.thegioididong.com/dtdd/iphone-15-p...</td>\n",
       "      <td>['M√°y n√≥ng r·∫•t t·ªá ,call cideo nh∆∞ mu·ªën n·ªï,ƒëang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>https://www.thegioididong.com/dtdd/iphone-15-p...</td>\n",
       "      <td>['T·ªët', 'M√°y m·ªõi mua s√†i mau n√≥ng m√°y qu√°.kh√¥n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>https://www.thegioididong.com/dtdd/iphone-15-p...</td>\n",
       "      <td>['R·∫•t tuy·ªát v·ªùi', 'Ok', 'R·∫•t T·ªët', 'M·ªçi th·ª© r·∫•...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>https://fptshop.com.vn/dien-thoai/iphone-15-pr...</td>\n",
       "      <td>['M√¨nh m·ªõi mua tr·∫£ g√≥p. th·ªß t·ª•c nhanh m√† nh√¢n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>https://fptshop.com.vn/dien-thoai/iphone-15-pr...</td>\n",
       "      <td>['Th·ªß t·ª•c mua h√†ng nhanh, ƒë∆°n gi·∫£n nh√©', 'S·ª≠ d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>iPhone 14 Plus</td>\n",
       "      <td>https://fptshop.com.vn/dien-thoai/iphone-14-pl...</td>\n",
       "      <td>['m√†u xanh ƒë·∫πp l·∫Øm lu√¥n, c·∫ßm ch·∫Øc tay, m√†n h√¨n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>iPhone 14 Plus</td>\n",
       "      <td>https://fptshop.com.vn/dien-thoai/iphone-14-pl...</td>\n",
       "      <td>['m√°y d√πng kh√° t·ªët, kh√¥ng c√≥ g√¨ ƒë·ªÉ ch√™, pin c≈©...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Samsung Galaxy A34 5G</td>\n",
       "      <td>https://www.thegioididong.com/dtdd/samsung-gal...</td>\n",
       "      <td>['D√πng t·∫°m ƒë∆∞·ª£c.nh∆∞ng khi mua th√¨ k ƒëc k√®m s·∫°c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Samsung Galaxy A34 5G</td>\n",
       "      <td>https://www.thegioididong.com/dtdd/samsung-gal...</td>\n",
       "      <td>['M√¨nh s√†i g·∫ßn nƒÉm r kh sao m√† g·∫ßn tu·∫ßn nay c·ª©...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Samsung Galaxy A34 5G</td>\n",
       "      <td>https://fptshop.com.vn/dien-thoai/samsung-gala...</td>\n",
       "      <td>['Ko c√≥ g√¨ ƒë·ªÉ ch√™, nh∆∞ng ph·∫£i tr·∫£i nghi·ªám m·ªôt ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Name                                               Link  \\\n",
       "0        iPhone 15 Pro Max  https://www.thegioididong.com/dtdd/iphone-15-p...   \n",
       "1        iPhone 15 Pro Max  https://www.thegioididong.com/dtdd/iphone-15-p...   \n",
       "2        iPhone 15 Pro Max  https://www.thegioididong.com/dtdd/iphone-15-p...   \n",
       "3        iPhone 15 Pro Max  https://fptshop.com.vn/dien-thoai/iphone-15-pr...   \n",
       "4        iPhone 15 Pro Max  https://fptshop.com.vn/dien-thoai/iphone-15-pr...   \n",
       "..                     ...                                                ...   \n",
       "142         iPhone 14 Plus  https://fptshop.com.vn/dien-thoai/iphone-14-pl...   \n",
       "143         iPhone 14 Plus  https://fptshop.com.vn/dien-thoai/iphone-14-pl...   \n",
       "144  Samsung Galaxy A34 5G  https://www.thegioididong.com/dtdd/samsung-gal...   \n",
       "145  Samsung Galaxy A34 5G  https://www.thegioididong.com/dtdd/samsung-gal...   \n",
       "146  Samsung Galaxy A34 5G  https://fptshop.com.vn/dien-thoai/samsung-gala...   \n",
       "\n",
       "                                              Comments  \n",
       "0    ['M√°y n√≥ng r·∫•t t·ªá ,call cideo nh∆∞ mu·ªën n·ªï,ƒëang...  \n",
       "1    ['T·ªët', 'M√°y m·ªõi mua s√†i mau n√≥ng m√°y qu√°.kh√¥n...  \n",
       "2    ['R·∫•t tuy·ªát v·ªùi', 'Ok', 'R·∫•t T·ªët', 'M·ªçi th·ª© r·∫•...  \n",
       "3    ['M√¨nh m·ªõi mua tr·∫£ g√≥p. th·ªß t·ª•c nhanh m√† nh√¢n ...  \n",
       "4    ['Th·ªß t·ª•c mua h√†ng nhanh, ƒë∆°n gi·∫£n nh√©', 'S·ª≠ d...  \n",
       "..                                                 ...  \n",
       "142  ['m√†u xanh ƒë·∫πp l·∫Øm lu√¥n, c·∫ßm ch·∫Øc tay, m√†n h√¨n...  \n",
       "143  ['m√°y d√πng kh√° t·ªët, kh√¥ng c√≥ g√¨ ƒë·ªÉ ch√™, pin c≈©...  \n",
       "144  ['D√πng t·∫°m ƒë∆∞·ª£c.nh∆∞ng khi mua th√¨ k ƒëc k√®m s·∫°c...  \n",
       "145  ['M√¨nh s√†i g·∫ßn nƒÉm r kh sao m√† g·∫ßn tu·∫ßn nay c·ª©...  \n",
       "146  ['Ko c√≥ g√¨ ƒë·ªÉ ch√™, nh∆∞ng ph·∫£i tr·∫£i nghi·ªám m·ªôt ...  \n",
       "\n",
       "[147 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df1['Link'] = pre_df1['Link'].str.replace(r'^https://www\\.thegioididong\\.com/.*', 'TGDD', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df1.loc[pre_df1['Link'] != 'TGDD', 'Link'] = 'FPT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Link</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>TGDD</td>\n",
       "      <td>['M√°y n√≥ng r·∫•t t·ªá ,call cideo nh∆∞ mu·ªën n·ªï,ƒëang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>TGDD</td>\n",
       "      <td>['T·ªët', 'M√°y m·ªõi mua s√†i mau n√≥ng m√°y qu√°.kh√¥n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>TGDD</td>\n",
       "      <td>['R·∫•t tuy·ªát v·ªùi', 'Ok', 'R·∫•t T·ªët', 'M·ªçi th·ª© r·∫•...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>FPT</td>\n",
       "      <td>['M√¨nh m·ªõi mua tr·∫£ g√≥p. th·ªß t·ª•c nhanh m√† nh√¢n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>FPT</td>\n",
       "      <td>['Th·ªß t·ª•c mua h√†ng nhanh, ƒë∆°n gi·∫£n nh√©', 'S·ª≠ d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>iPhone 14 Plus</td>\n",
       "      <td>FPT</td>\n",
       "      <td>['m√†u xanh ƒë·∫πp l·∫Øm lu√¥n, c·∫ßm ch·∫Øc tay, m√†n h√¨n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>iPhone 14 Plus</td>\n",
       "      <td>FPT</td>\n",
       "      <td>['m√°y d√πng kh√° t·ªët, kh√¥ng c√≥ g√¨ ƒë·ªÉ ch√™, pin c≈©...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Samsung Galaxy A34 5G</td>\n",
       "      <td>TGDD</td>\n",
       "      <td>['D√πng t·∫°m ƒë∆∞·ª£c.nh∆∞ng khi mua th√¨ k ƒëc k√®m s·∫°c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Samsung Galaxy A34 5G</td>\n",
       "      <td>TGDD</td>\n",
       "      <td>['M√¨nh s√†i g·∫ßn nƒÉm r kh sao m√† g·∫ßn tu·∫ßn nay c·ª©...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Samsung Galaxy A34 5G</td>\n",
       "      <td>FPT</td>\n",
       "      <td>['Ko c√≥ g√¨ ƒë·ªÉ ch√™, nh∆∞ng ph·∫£i tr·∫£i nghi·ªám m·ªôt ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Name  Link  \\\n",
       "0        iPhone 15 Pro Max  TGDD   \n",
       "1        iPhone 15 Pro Max  TGDD   \n",
       "2        iPhone 15 Pro Max  TGDD   \n",
       "3        iPhone 15 Pro Max   FPT   \n",
       "4        iPhone 15 Pro Max   FPT   \n",
       "..                     ...   ...   \n",
       "142         iPhone 14 Plus   FPT   \n",
       "143         iPhone 14 Plus   FPT   \n",
       "144  Samsung Galaxy A34 5G  TGDD   \n",
       "145  Samsung Galaxy A34 5G  TGDD   \n",
       "146  Samsung Galaxy A34 5G   FPT   \n",
       "\n",
       "                                              Comments  \n",
       "0    ['M√°y n√≥ng r·∫•t t·ªá ,call cideo nh∆∞ mu·ªën n·ªï,ƒëang...  \n",
       "1    ['T·ªët', 'M√°y m·ªõi mua s√†i mau n√≥ng m√°y qu√°.kh√¥n...  \n",
       "2    ['R·∫•t tuy·ªát v·ªùi', 'Ok', 'R·∫•t T·ªët', 'M·ªçi th·ª© r·∫•...  \n",
       "3    ['M√¨nh m·ªõi mua tr·∫£ g√≥p. th·ªß t·ª•c nhanh m√† nh√¢n ...  \n",
       "4    ['Th·ªß t·ª•c mua h√†ng nhanh, ƒë∆°n gi·∫£n nh√©', 'S·ª≠ d...  \n",
       "..                                                 ...  \n",
       "142  ['m√†u xanh ƒë·∫πp l·∫Øm lu√¥n, c·∫ßm ch·∫Øc tay, m√†n h√¨n...  \n",
       "143  ['m√°y d√πng kh√° t·ªët, kh√¥ng c√≥ g√¨ ƒë·ªÉ ch√™, pin c≈©...  \n",
       "144  ['D√πng t·∫°m ƒë∆∞·ª£c.nh∆∞ng khi mua th√¨ k ƒëc k√®m s·∫°c...  \n",
       "145  ['M√¨nh s√†i g·∫ßn nƒÉm r kh sao m√† g·∫ßn tu·∫ßn nay c·ª©...  \n",
       "146  ['Ko c√≥ g√¨ ƒë·ªÉ ch√™, nh∆∞ng ph·∫£i tr·∫£i nghi·ªám m·ªôt ...  \n",
       "\n",
       "[147 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(df, name):\n",
    "    comments = df.loc[df['Name'] == name, 'Comments'].values\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name = get_comments(pre_df1, 'iPhone 15 Pro Max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(df, name):\n",
    "    comments = df.loc[df['Name'] == name, 'Comments'].values\n",
    "    return comments\n",
    "def list_comments(l_comments):\n",
    "    all_comments = []\n",
    "    for i in range(len(l_comments)):\n",
    "        split_comments = ast.literal_eval(l_comments[i])\n",
    "        for comment in split_comments:\n",
    "            all_comments.append(comment)\n",
    "    return all_comments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_com = list_comments(product_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "class SVCModel():\n",
    "    def __init__(self, kernel='rbf', C=1.0, gamma='scale', attribute=None):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        self.gamma = gamma\n",
    "        self.svc = None\n",
    "        self.attribute = attribute\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.svc = SVC(kernel=self.kernel, C=self.C, gamma=self.gamma, probability=True)\n",
    "        self.svc.fit(X, y[self.attribute])\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.svc is None:\n",
    "            raise ValueError(\"SVC model has not been trained yet. Please call 'fit' first.\")\n",
    "        return self.svc.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if self.svc is None:\n",
    "            raise ValueError(\"SVC model has not been trained yet. Please call 'fit' first.\")\n",
    "        return self.svc.predict_proba(X)\n",
    "    \n",
    "    def calculate_accuracy_score(self, X, y, y_pred=None):\n",
    "        if self.svc is None:\n",
    "            raise ValueError(\"SVC model has not been trained yet. Please call 'fit' first.\")\n",
    "        \n",
    "        if y_pred is not None:\n",
    "            accuracy = accuracy_score(y[self.attribute], y_pred)\n",
    "            return accuracy\n",
    "        \n",
    "        y_pred = self.predict(X)\n",
    "        accuracy = accuracy_score(y[self.attribute], y_pred)\n",
    "        return accuracy\n",
    "    \n",
    "    def calculate_f1_score(self, X, y, y_pred=None):\n",
    "        if self.svc is None:\n",
    "            raise ValueError(\"SVC model has not been trained yet. Please call 'fit' first.\")\n",
    "\n",
    "        if y_pred is not None:\n",
    "            if y[self.attribute].nunique() < 3:\n",
    "                f1 = f1_score(y[self.attribute], y_pred)\n",
    "            else:\n",
    "                f1 = f1_score(y[self.attribute], y_pred, average='micro')\n",
    "            return f1\n",
    "\n",
    "        y_pred = self.predict(X)\n",
    "        if y[self.attribute].nunique() < 3:\n",
    "            f1 = f1_score(y[self.attribute], y_pred)\n",
    "        else:\n",
    "            f1 = f1_score(y[self.attribute], y_pred, average='micro')\n",
    "        return f1\n",
    "    \n",
    "    def calculate_precision_score(self, X, y, y_pred=None):\n",
    "        if self.svc is None:\n",
    "            raise ValueError(\"SVC model has not been trained yet. Please call 'fit' first.\")\n",
    "        \n",
    "        if y_pred is not None:\n",
    "            if y[self.attribute].nunique() < 3:\n",
    "                precision = precision_score(y[self.attribute], y_pred)\n",
    "            else:\n",
    "                precision = precision_score(y[self.attribute], y_pred, average='micro')\n",
    "            return precision\n",
    "        \n",
    "        y_pred = self.predict(X)\n",
    "        if y[self.attribute].nunique() < 3:\n",
    "            precision = precision_score(y[self.attribute], y_pred)\n",
    "        else:\n",
    "            precision = precision_score(y[self.attribute], y_pred, average='micro')\n",
    "        return precision\n",
    "    \n",
    "    def calculate_recall_score(self, X, y, y_pred=None):\n",
    "        if self.svc is None:\n",
    "            raise ValueError(\"SVC model has not been trained yet. Please call 'fit' first.\")\n",
    "        \n",
    "        if y_pred is not None:\n",
    "            if y[self.attribute].nunique() < 3:\n",
    "                recall = recall_score(y[self.attribute], y_pred)\n",
    "            else:\n",
    "                recall = recall_score(y[self.attribute], y_pred, average='micro')\n",
    "            return recall\n",
    "        \n",
    "        y_pred = self.predict(X)\n",
    "        if y[self.attribute].nunique() < 3:\n",
    "            recall = recall_score(y[self.attribute], y_pred)\n",
    "        else:\n",
    "            recall = recall_score(y[self.attribute], y_pred, average='micro')\n",
    "        return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess.py\n",
    "import regex as re\n",
    "from underthesea import word_tokenize\n",
    "import string\n",
    "import codecs\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "\n",
    "# remove html tags\n",
    "def remove_html(txt):\n",
    "    return re.sub(r'<[^>]*>', '', txt)\n",
    "\n",
    "# unicode stardard\n",
    "uniChars = \"√†√°·∫£√£·∫°√¢·∫ß·∫•·∫©·∫´·∫≠ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªáƒë√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµ√Ä√Å·∫¢√É·∫†√Ç·∫¶·∫§·∫®·∫™·∫¨ƒÇ·∫∞·∫Æ·∫≤·∫¥·∫∂√à√â·∫∫·∫º·∫∏√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜƒê√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªí·ªê·ªî·ªñ·ªò∆†·ªú·ªö·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª™·ª®·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥√ÇƒÇƒê√î∆†∆Ø\"\n",
    "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
    "\n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'aÃÄ|aÃÅ|aÃâ|aÃÉ|aÃ£|√¢ÃÄ|√¢ÃÅ|√¢Ãâ|√¢ÃÉ|√¢Ã£|ƒÉÃÄ|ƒÉÃÅ|ƒÉÃâ|ƒÉÃÉ|ƒÉÃ£|eÃÄ|eÃÅ|eÃâ|eÃÉ|eÃ£|√™ÃÄ|√™ÃÅ|√™Ãâ|√™ÃÉ|√™Ã£|iÃÄ|iÃÅ|iÃâ|iÃÉ|iÃ£|oÃÄ|oÃÅ|oÃâ|oÃÉ|oÃ£|√¥ÃÄ|√¥ÃÅ|√¥Ãâ|√¥ÃÉ|√¥Ã£|∆°ÃÄ|∆°ÃÅ|∆°Ãâ|∆°ÃÉ|∆°Ã£|uÃÄ|uÃÅ|uÃâ|uÃÉ|uÃ£|∆∞ÃÄ|∆∞ÃÅ|∆∞Ãâ|∆∞ÃÉ|∆∞Ã£|yÃÄ|yÃÅ|yÃâ|yÃÉ|yÃ£|AÃÄ|AÃÅ|AÃâ|AÃÉ|AÃ£|√ÇÃÄ|√ÇÃÅ|√ÇÃâ|√ÇÃÉ|√ÇÃ£|ƒÇÃÄ|ƒÇÃÅ|ƒÇÃâ|ƒÇÃÉ|ƒÇÃ£|EÃÄ|EÃÅ|EÃâ|EÃÉ|EÃ£|√äÃÄ|√äÃÅ|√äÃâ|√äÃÉ|√äÃ£|IÃÄ|IÃÅ|IÃâ|IÃÉ|IÃ£|OÃÄ|OÃÅ|OÃâ|OÃÉ|OÃ£|√îÃÄ|√îÃÅ|√îÃâ|√îÃÉ|√îÃ£|∆†ÃÄ|∆†ÃÅ|∆†Ãâ|∆†ÃÉ|∆†Ã£|UÃÄ|UÃÅ|UÃâ|UÃÉ|UÃ£|∆ØÃÄ|∆ØÃÅ|∆ØÃâ|∆ØÃÉ|∆ØÃ£|YÃÄ|YÃÅ|YÃâ|YÃÉ|YÃ£'.split(\n",
    "        '|')\n",
    "    charutf8 = \"√†|√°|·∫£|√£|·∫°|·∫ß|·∫•|·∫©|·∫´|·∫≠|·∫±|·∫Ø|·∫≥|·∫µ|·∫∑|√®|√©|·∫ª|·∫Ω|·∫π|·ªÅ|·∫ø|·ªÉ|·ªÖ|·ªá|√¨|√≠|·ªâ|ƒ©|·ªã|√≤|√≥|·ªè|√µ|·ªç|·ªì|·ªë|·ªï|·ªó|·ªô|·ªù|·ªõ|·ªü|·ª°|·ª£|√π|√∫|·ªß|≈©|·ª•|·ª´|·ª©|·ª≠|·ªØ|·ª±|·ª≥|√Ω|·ª∑|·ªπ|·ªµ|√Ä|√Å|·∫¢|√É|·∫†|·∫¶|·∫§|·∫®|·∫™|·∫¨|·∫∞|·∫Æ|·∫≤|·∫¥|·∫∂|√à|√â|·∫∫|·∫º|·∫∏|·ªÄ|·∫æ|·ªÇ|·ªÑ|·ªÜ|√å|√ç|·ªà|ƒ®|·ªä|√í|√ì|·ªé|√ï|·ªå|·ªí|·ªê|·ªî|·ªñ|·ªò|·ªú|·ªö|·ªû|·ª†|·ª¢|√ô|√ö|·ª¶|≈®|·ª§|·ª™|·ª®|·ª¨|·ªÆ|·ª∞|·ª≤|√ù|·ª∂|·ª∏|·ª¥\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "dicchar = loaddicchar()\n",
    "\n",
    "def convert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'aÃÄ|aÃÅ|aÃâ|aÃÉ|aÃ£|√¢ÃÄ|√¢ÃÅ|√¢Ãâ|√¢ÃÉ|√¢Ã£|ƒÉÃÄ|ƒÉÃÅ|ƒÉÃâ|ƒÉÃÉ|ƒÉÃ£|eÃÄ|eÃÅ|eÃâ|eÃÉ|eÃ£|√™ÃÄ|√™ÃÅ|√™Ãâ|√™ÃÉ|√™Ã£|iÃÄ|iÃÅ|iÃâ|iÃÉ|iÃ£|oÃÄ|oÃÅ|oÃâ|oÃÉ|oÃ£|√¥ÃÄ|√¥ÃÅ|√¥Ãâ|√¥ÃÉ|√¥Ã£|∆°ÃÄ|∆°ÃÅ|∆°Ãâ|∆°ÃÉ|∆°Ã£|uÃÄ|uÃÅ|uÃâ|uÃÉ|uÃ£|∆∞ÃÄ|∆∞ÃÅ|∆∞Ãâ|∆∞ÃÉ|∆∞Ã£|yÃÄ|yÃÅ|yÃâ|yÃÉ|yÃ£|AÃÄ|AÃÅ|AÃâ|AÃÉ|AÃ£|√ÇÃÄ|√ÇÃÅ|√ÇÃâ|√ÇÃÉ|√ÇÃ£|ƒÇÃÄ|ƒÇÃÅ|ƒÇÃâ|ƒÇÃÉ|ƒÇÃ£|EÃÄ|EÃÅ|EÃâ|EÃÉ|EÃ£|√äÃÄ|√äÃÅ|√äÃâ|√äÃÉ|√äÃ£|IÃÄ|IÃÅ|IÃâ|IÃÉ|IÃ£|OÃÄ|OÃÅ|OÃâ|OÃÉ|OÃ£|√îÃÄ|√îÃÅ|√îÃâ|√îÃÉ|√îÃ£|∆†ÃÄ|∆†ÃÅ|∆†Ãâ|∆†ÃÉ|∆†Ã£|UÃÄ|UÃÅ|UÃâ|UÃÉ|UÃ£|∆ØÃÄ|∆ØÃÅ|∆ØÃâ|∆ØÃÉ|∆ØÃ£|YÃÄ|YÃÅ|YÃâ|YÃÉ|YÃ£',\n",
    "        lambda x: dicchar[x.group()], txt)\n",
    "\n",
    "# diacritic standard\n",
    "vowels_to_ids = {}\n",
    "vowels_table = [\n",
    "    ['a', '√†', '√°', '·∫£', '√£', '·∫°', 'a' ],\n",
    "    ['ƒÉ', '·∫±', '·∫Ø', '·∫≥', '·∫µ', '·∫∑', 'aw'],\n",
    "    ['√¢', '·∫ß', '·∫•', '·∫©', '·∫´', '·∫≠', 'aa'],\n",
    "    ['e', '√®', '√©', '·∫ª', '·∫Ω', '·∫π', 'e' ],\n",
    "    ['√™', '·ªÅ', '·∫ø', '·ªÉ', '·ªÖ', '·ªá', 'ee'],\n",
    "    ['i', '√¨', '√≠', '·ªâ', 'ƒ©', '·ªã', 'i' ],\n",
    "    ['o', '√≤', '√≥', '·ªè', '√µ', '·ªç', 'o' ],\n",
    "    ['√¥', '·ªì', '·ªë', '·ªï', '·ªó', '·ªô', 'oo'],\n",
    "    ['∆°', '·ªù', '·ªõ', '·ªü', '·ª°', '·ª£', 'ow'],\n",
    "    ['u', '√π', '√∫', '·ªß', '≈©', '·ª•', 'u' ],\n",
    "    ['∆∞', '·ª´', '·ª©', '·ª≠', '·ªØ', '·ª±', 'uw'],\n",
    "    ['y', '·ª≥', '√Ω', '·ª∑', '·ªπ', '·ªµ', 'y' ]\n",
    "]\n",
    "\n",
    "for i in range(len(vowels_table)):\n",
    "    for j in range(len(vowels_table[i]) - 1):\n",
    "        vowels_to_ids[vowels_table[i][j]] = (i, j)\n",
    "\n",
    "def is_valid_vietnamese_word(word):\n",
    "    chars = list(word)\n",
    "    vowel_indexes = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = vowels_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if vowel_indexes == -1: vowel_indexes = index\n",
    "            else:\n",
    "                if index - vowel_indexes != 1: return False\n",
    "                vowel_indexes = index\n",
    "    return True\n",
    "\n",
    "def standardize_word_typing(word):\n",
    "    if not is_valid_vietnamese_word(word): return word\n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    vowel_indexes = []\n",
    "    qu_or_gi = False\n",
    "\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = vowels_to_ids.get(char, (-1, -1))\n",
    "        if x == -1: continue\n",
    "        elif x == 9:  # check qu\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:  # check gi\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = vowels_table[x][0]\n",
    "\n",
    "        if not qu_or_gi or index != 1:\n",
    "            vowel_indexes.append(index)\n",
    "\n",
    "    if len(vowel_indexes) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = vowels_to_ids.get(chars[1])\n",
    "                chars[1] = vowels_table[x][dau_cau]\n",
    "            else:\n",
    "                x, y = vowels_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1: chars[2] = vowels_table[x][dau_cau]\n",
    "                else: chars[1] = vowels_table[5][dau_cau] if chars[1] == 'i' else vowels_table[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    "\n",
    "    for index in vowel_indexes:\n",
    "        x, y = vowels_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # √™, ∆°\n",
    "            chars[index] = vowels_table[x][dau_cau]\n",
    "            return ''.join(chars)\n",
    "\n",
    "    if len(vowel_indexes) == 2:\n",
    "        if vowel_indexes[-1] == len(chars) - 1:\n",
    "            x, y = vowels_to_ids[chars[vowel_indexes[0]]]\n",
    "            chars[vowel_indexes[0]] = vowels_table[x][dau_cau]\n",
    "        else:\n",
    "            x, y = vowels_to_ids[chars[vowel_indexes[1]]]\n",
    "            chars[vowel_indexes[1]] = vowels_table[x][dau_cau]\n",
    "    else:\n",
    "        x, y = vowels_to_ids[chars[vowel_indexes[1]]]\n",
    "        chars[vowel_indexes[1]] = vowels_table[x][dau_cau]\n",
    "    return ''.join(chars)\n",
    "\n",
    "def standardize_sentence_typing(text):\n",
    "    words = text.lower().split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
    "        if len(cw) == 3: cw[1] = standardize_word_typing(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)\n",
    "\n",
    "# remove unnecessary characters\n",
    "def remove_unnecessary(text):\n",
    "    text = re.sub(r'[^\\s\\w√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√≠√¨·ªâƒ©·ªã√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµƒë√Å√Ä·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√â√à·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√ì√í·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ç√å·ªàƒ®·ªä√ö√ô·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞√ù·ª≤·ª∂·ª∏·ª¥ƒê_]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "# special wordings in reviews (e.g., emoji,...)\n",
    "def normalize_money(sent):\n",
    "    return re.sub(r'[0-9]+[.,0-9][k-m-b]', 'gi√°', sent)\n",
    "\n",
    "def normalize_hastag(sent):\n",
    "    return re.sub(r'#+\\w+', 'tag', sent)\n",
    "\n",
    "def normalize_website(sent):\n",
    "    result = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'website', sent)\n",
    "    return re.sub(r'\\w+(\\.(com|vn|me))+((\\/+([\\.\\w\\_\\-]+)?)+)?', 'website', result)\n",
    "\n",
    "def nomalize_emoji(sent):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                u\"\\U00002702-\\U000027B0\"\n",
    "                u\"\\U000024C2-\\U0001F251\"\n",
    "                u\"\\U0001f926-\\U0001f937\"\n",
    "                u'\\U00010000-\\U0010ffff'\n",
    "                u\"\\u200d\"\n",
    "                u\"\\u2640-\\u2642\"\n",
    "                u\"\\u2600-\\u2B55\"\n",
    "                u\"\\u23cf\"\n",
    "                u\"\\u23e9\"\n",
    "                u\"\\u231a\"\n",
    "                u\"\\u3030\"\n",
    "                u\"\\ufe0f\"\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', sent)\n",
    "\n",
    "def normalize_elongate(sent):\n",
    "    patern = r'(.)\\1{1,}'\n",
    "    result = sent\n",
    "    while(re.search(patern, result) != None):\n",
    "        repeat_char = re.search(patern, result)\n",
    "        result = result.replace(repeat_char[0], repeat_char[1])\n",
    "    return result\n",
    "\n",
    "def remove_numbers(sent):\n",
    "    return re.sub(r'[0-9]+', '', sent)\n",
    "\n",
    "def normalize_acronyms(sent):\n",
    "    text = sent\n",
    "    replace_list = {\n",
    "            \"√≤a\": \"o√†\",\n",
    "        \"√ía\": \"O√†\",\n",
    "        \"√íA\": \"O√Ä\",\n",
    "        \"√≥a\": \"o√°\",\n",
    "        \"√ìa\": \"O√°\",\n",
    "        \"√ìA\": \"O√Å\",\n",
    "        \"·ªèa\": \"o·∫£\",\n",
    "        \"·ªéa\": \"O·∫£\",\n",
    "        \"·ªéA\": \"O·∫¢\",\n",
    "        \"√µa\": \"o√£\",\n",
    "        \"√ïa\": \"O√£\",\n",
    "        \"√ïA\": \"O√É\",\n",
    "        \"·ªça\": \"o·∫°\",\n",
    "        \"·ªåa\": \"O·∫°\",\n",
    "        \"·ªåA\": \"O·∫†\",\n",
    "        \"√≤e\": \"o√®\",\n",
    "        \"√íe\": \"O√®\",\n",
    "        \"√íE\": \"O√à\",\n",
    "        \"√≥e\": \"o√©\",\n",
    "        \"√ìe\": \"O√©\",\n",
    "        \"√ìE\": \"O√â\",\n",
    "        \"·ªèe\": \"o·∫ª\",\n",
    "        \"·ªée\": \"O·∫ª\",\n",
    "        \"·ªéE\": \"O·∫∫\",\n",
    "        \"√µe\": \"o·∫Ω\",\n",
    "        \"√ïe\": \"O·∫Ω\",\n",
    "        \"√ïE\": \"O·∫º\",\n",
    "        \"·ªçe\": \"o·∫π\",\n",
    "        \"·ªåe\": \"O·∫π\",\n",
    "        \"·ªåE\": \"O·∫∏\",\n",
    "        \"√πy\": \"u·ª≥\",\n",
    "        \"√ôy\": \"U·ª≥\",\n",
    "        \"√ôY\": \"U·ª≤\",\n",
    "        \"√∫y\": \"u√Ω\",\n",
    "        \"√öy\": \"U√Ω\",\n",
    "        \"√öY\": \"U√ù\",\n",
    "        \"·ªßy\": \"u·ª∑\",\n",
    "        \"·ª¶y\": \"U·ª∑\",\n",
    "        \"·ª¶Y\": \"U·ª∂\",\n",
    "        \"≈©y\": \"u·ªπ\",\n",
    "        \"≈®y\": \"U·ªπ\",\n",
    "        \"≈®Y\": \"U·ª∏\",\n",
    "        \"·ª•y\": \"u·ªµ\",\n",
    "        \"·ª§y\": \"U·ªµ\",\n",
    "        \"·ª§Y\": \"U·ª¥\",\n",
    "        'aÃâ': '·∫£', '√¥ÃÅ': '·ªë', 'u¬¥': '·ªë','√¥ÃÉ': '·ªó', '√¥ÃÄ': '·ªì', '√¥Ãâ': '·ªï', '√¢ÃÅ': '·∫•', '√¢ÃÉ': '·∫´', '√¢Ãâ': '·∫©',\n",
    "        '√¢ÃÄ': '·∫ß', 'oÃâ': '·ªè', '√™ÃÄ': '·ªÅ','√™ÃÉ': '·ªÖ', 'ƒÉÃÅ': '·∫Ø', 'uÃâ': '·ªß', '√™ÃÅ': '·∫ø', '∆°Ãâ': '·ªü', 'iÃâ': '·ªâ',\n",
    "        'eÃâ': '·∫ª', '√†k': u' √† ','aÀã': '√†', 'iÀã': '√¨', 'ƒÉ¬¥': '·∫Ø','∆∞Ãâ': '·ª≠', 'eÀú': '·∫Ω', 'yÀú': '·ªπ', 'a¬¥': '√°',\n",
    "            #Quy c√°c icon v·ªÅ 2 lo·∫°i emoj: T√≠ch c·ª±c ho·∫∑c ti√™u c·ª±c\n",
    "            \"üëπ\": \"Ti√™u c·ª±c\", \"üëª\": \"T√≠ch c·ª±c\", \"üíÉ\": \"T√≠ch c·ª±c\",'ü§ô': ' T√≠ch c·ª±c ', 'üëç': ' T√≠ch c·ª±c ',\n",
    "            \"üíÑ\": \"T√≠ch c·ª±c\", \"üíé\": \"T√≠ch c·ª±c\", \"üí©\": \"T√≠ch c·ª±c\",\"üòï\": \"Ti√™u c·ª±c\", \"üò±\": \"Ti√™u c·ª±c\", \"üò∏\": \"T√≠ch c·ª±c\",\n",
    "            \"üòæ\": \"Ti√™u c·ª±c\", \"üö´\": \"Ti√™u c·ª±c\",  \"ü§¨\": \"Ti√™u c·ª±c\",\"üßö\": \"T√≠ch c·ª±c\", \"üß°\": \"T√≠ch c·ª±c\",'üê∂':' T√≠ch c·ª±c ',\n",
    "            'üëé': ' Ti√™u c·ª±c ', 'üò£': ' Ti√™u c·ª±c ','‚ú®': ' T√≠ch c·ª±c ', '‚ù£': ' T√≠ch c·ª±c ','‚òÄ': ' T√≠ch c·ª±c ',\n",
    "            '‚ô•': ' T√≠ch c·ª±c ', 'ü§©': ' T√≠ch c·ª±c ', 'like': ' T√≠ch c·ª±c ', 'üíå': ' T√≠ch c·ª±c ',\n",
    "            'ü§£': ' T√≠ch c·ª±c ', 'üñ§': ' T√≠ch c·ª±c ', 'ü§§': ' T√≠ch c·ª±c ', ':(': ' Ti√™u c·ª±c ', 'üò¢': ' Ti√™u c·ª±c ',\n",
    "            '‚ù§': ' T√≠ch c·ª±c ', 'üòç': ' T√≠ch c·ª±c ', 'üòò': ' T√≠ch c·ª±c ', 'üò™': ' Ti√™u c·ª±c ', 'üòä': ' T√≠ch c·ª±c ',\n",
    "            '?': ' ? ', 'üòÅ': ' T√≠ch c·ª±c ', 'üíñ': ' T√≠ch c·ª±c ', 'üòü': ' Ti√™u c·ª±c ', 'üò≠': ' Ti√™u c·ª±c ',\n",
    "            'üíØ': ' T√≠ch c·ª±c ', 'üíó': ' T√≠ch c·ª±c ', '‚ô°': ' T√≠ch c·ª±c ', 'üíú': ' T√≠ch c·ª±c ', 'ü§ó': ' T√≠ch c·ª±c ',\n",
    "            '^^': ' T√≠ch c·ª±c ', 'üò®': ' Ti√™u c·ª±c ', '‚ò∫': ' T√≠ch c·ª±c ', 'üíã': ' T√≠ch c·ª±c ', 'üëå': ' T√≠ch c·ª±c ',\n",
    "            'üòñ': ' Ti√™u c·ª±c ', 'üòÄ': ' T√≠ch c·ª±c ', ':((': ' Ti√™u c·ª±c ', 'üò°': ' Ti√™u c·ª±c ', 'üò†': ' Ti√™u c·ª±c ',\n",
    "            'üòí': ' Ti√™u c·ª±c ', 'üôÇ': ' T√≠ch c·ª±c ', 'üòè': ' Ti√™u c·ª±c ', 'üòù': ' T√≠ch c·ª±c ', 'üòÑ': ' T√≠ch c·ª±c ',\n",
    "            'üòô': ' T√≠ch c·ª±c ', 'üò§': ' Ti√™u c·ª±c ', 'üòé': ' T√≠ch c·ª±c ', 'üòÜ': ' T√≠ch c·ª±c ', 'üíö': ' T√≠ch c·ª±c ',\n",
    "            '‚úå': ' T√≠ch c·ª±c ', 'üíï': ' T√≠ch c·ª±c ', 'üòû': ' Ti√™u c·ª±c ', 'üòì': ' Ti√™u c·ª±c ', 'Ô∏èüÜóÔ∏è': ' T√≠ch c·ª±c ',\n",
    "            'üòâ': ' T√≠ch c·ª±c ', 'üòÇ': ' T√≠ch c·ª±c ', ':v': '  T√≠ch c·ª±c ', '=))': '  T√≠ch c·ª±c ', 'üòã': ' T√≠ch c·ª±c ',\n",
    "            'üíì': ' T√≠ch c·ª±c ', 'üòê': ' Ti√™u c·ª±c ', ':3': ' T√≠ch c·ª±c ', 'üò´': ' Ti√™u c·ª±c ', 'üò•': ' Ti√™u c·ª±c ',\n",
    "            'üòÉ': ' T√≠ch c·ª±c ', 'üò¨': ' üò¨ ', 'üòå': ' üòå ', 'üíõ': ' T√≠ch c·ª±c ', 'ü§ù': ' T√≠ch c·ª±c ', 'üéà': ' T√≠ch c·ª±c ',\n",
    "            'üòó': ' T√≠ch c·ª±c ', 'ü§î': ' Ti√™u c·ª±c ', 'üòë': ' Ti√™u c·ª±c ', 'üî•': ' Ti√™u c·ª±c ', 'üôè': ' Ti√™u c·ª±c ',\n",
    "            'üÜó': ' T√≠ch c·ª±c ', 'üòª': ' T√≠ch c·ª±c ', 'üíô': ' T√≠ch c·ª±c ', 'üíü': ' T√≠ch c·ª±c ',\n",
    "            'üòö': ' T√≠ch c·ª±c ', '‚ùå': ' Ti√™u c·ª±c ', 'üëè': ' T√≠ch c·ª±c ', ';)': ' T√≠ch c·ª±c ', '<3': ' T√≠ch c·ª±c ',\n",
    "            'üåù': ' T√≠ch c·ª±c ',  'üå∑': ' T√≠ch c·ª±c ', 'üå∏': ' T√≠ch c·ª±c ', 'üå∫': ' T√≠ch c·ª±c ',\n",
    "            'üåº': ' T√≠ch c·ª±c ', 'üçì': ' T√≠ch c·ª±c ', 'üêÖ': ' T√≠ch c·ª±c ', 'üêæ': ' T√≠ch c·ª±c ', 'üëâ': ' T√≠ch c·ª±c ',\n",
    "            'üíê': ' T√≠ch c·ª±c ', 'üíû': ' T√≠ch c·ª±c ', 'üí•': ' T√≠ch c·ª±c ', 'üí™': ' T√≠ch c·ª±c ',\n",
    "            'üí∞': ' T√≠ch c·ª±c ',  'üòá': ' T√≠ch c·ª±c ', 'üòõ': ' T√≠ch c·ª±c ', 'üòú': ' T√≠ch c·ª±c ',\n",
    "            'üôÉ': ' T√≠ch c·ª±c ', 'ü§ë': ' T√≠ch c·ª±c ', 'ü§™': ' T√≠ch c·ª±c ','‚òπ': ' Ti√™u c·ª±c ',  'üíÄ': ' Ti√™u c·ª±c ',\n",
    "            'üòî': ' Ti√™u c·ª±c ', 'üòß': ' Ti√™u c·ª±c ', 'üò©': ' Ti√™u c·ª±c ', 'üò∞': ' Ti√™u c·ª±c ', 'üò≥': ' Ti√™u c·ª±c ',\n",
    "            'üòµ': ' Ti√™u c·ª±c ', 'üò∂': ' Ti√™u c·ª±c ', 'üôÅ': ' Ti√™u c·ª±c ',\n",
    "            #Chu·∫©n h√≥a 1 s·ªë sentiment words/English words\n",
    "            ':))': '  T√≠ch c·ª±c ', ':)': ' T√≠ch c·ª±c ', '√¥ k√™i': ' ok ', 'okie': ' ok ', ' o k√™ ': ' ok ',\n",
    "            'okey': ' ok ', '√¥k√™': ' ok ', 'oki': ' ok ', ' oke ':  ' ok ',' okay':' ok ','ok√™':' ok ',\n",
    "            ' tks ': u' c√°m ∆°n ', 'thks': u' c√°m ∆°n ', 'thanks': u' c√°m ∆°n ', 'ths': u' c√°m ∆°n ', 'thank': u' c√°m ∆°n ',\n",
    "            '‚≠ê': 'star ', '*': 'star ', 'üåü': 'star ', 'üéâ': u' T√≠ch c·ª±c ',\n",
    "            'kg ': u' kh√¥ng ','not': u' kh√¥ng ', u' kg ': u' kh√¥ng ', '\"k ': u' kh√¥ng ',' kh ':u' kh√¥ng ','k√¥':u' kh√¥ng ','hok':u' kh√¥ng ',' kp ': u' kh√¥ng ph·∫£i ',u' k√¥ ': u' kh√¥ng ', '\"ko ': u' kh√¥ng ', u' ko ': u' kh√¥ng ', u' k ': u' kh√¥ng ', 'khong': u' kh√¥ng ', u' hok ': u' kh√¥ng ',\n",
    "            'he he': ' T√≠ch c·ª±c ','hehe': ' T√≠ch c·ª±c ','hihi': ' T√≠ch c·ª±c ', 'haha': ' T√≠ch c·ª±c ', 'hjhj': ' T√≠ch c·ª±c ',\n",
    "            ' lol ': ' Ti√™u c·ª±c ',' cc ': ' Ti√™u c·ª±c ','cute': u' d·ªÖ th∆∞∆°ng ','huhu': ' Ti√™u c·ª±c ', ' vs ': u' v·ªõi ', 'wa': ' qu√° ', 'w√°': u' qu√°', 'j': u' g√¨ ', '‚Äú': ' ',\n",
    "            ' sz ': u' c·ª° ', 'size': u' c·ª° ', u' ƒëx ': u' ƒë∆∞·ª£c ', 'dk': u' ƒë∆∞·ª£c  ', 'dc': u' ƒë∆∞·ª£c ', 'ƒëk': u' ƒë∆∞·ª£c ',\n",
    "            'ƒëc': u' ƒë∆∞·ª£c ','authentic': u' chu·∫©n ch√≠nh h√£ng ',u' aut ': u' chu·∫©n ch√≠nh h√£ng ', u' auth ': u' chu·∫©n ch√≠nh h√£ng ', 'thick': u' T√≠ch c·ª±c ', 'store': u' c·ª≠a h√†ng ',\n",
    "            'shop': u' c·ª≠a h√†ng ', 'sp': u' s·∫£n ph·∫©m ', 'gud': u' t·ªët ','god': u' t·ªët ','wel done':' t·ªët ', 'good': u' t·ªët ', 'g√∫t': u' t·ªët ',\n",
    "            's·∫•u': u' x·∫•u ','gut': u' t·ªët ', u' tot ': u' t·ªët ', u' nice ': u' t·ªët ', 'perfect': 'r·∫•t t·ªët', 'bt': u' b√¨nh th∆∞·ªùng ',\n",
    "            'time': u' th·ªùi gian ', 'q√°': u' qu√° ', u' ship ': u' giao h√†ng ', u' m ': u' m√¨nh ', u' mik ': u' m√¨nh ',\n",
    "            '√™Ãâ': '·ªÉ', 'product': 's·∫£n ph·∫©m', 'quality': 'ch·∫•t l∆∞·ª£ng','chat':' ch·∫•t ', 'excelent': 'ho√†n h·∫£o', 'bad': 't·ªá','fresh': ' t∆∞∆°i ','sad': ' t·ªá ',\n",
    "            'date': u' h·∫°n s·ª≠ d·ª•ng ', 'hsd': u' h·∫°n s·ª≠ d·ª•ng ','quickly': u' nhanh ', 'quick': u' nhanh ','fast': u' nhanh ','delivery': u' giao h√†ng ',u' s√≠p ': u' giao h√†ng ',\n",
    "            'beautiful': u' ƒë·∫πp tuy·ªát v·ªùi ', u' tl ': u' tr·∫£ l·ªùi ', u' r ': u' r·ªìi ', u' shopE ': u' c·ª≠a h√†ng ',u' order ': u' ƒë·∫∑t h√†ng ',\n",
    "            'ch·∫•t lg': u' ch·∫•t l∆∞·ª£ng ',u' sd ': u' s·ª≠ d·ª•ng ',u' dt ': u' ƒëi·ªán tho·∫°i ',u' nt ': u' nh·∫Øn tin ',u' tl ': u' tr·∫£ l·ªùi ',u' s√†i ': u' x√†i ',u'bjo':u' bao gi·ªù ',\n",
    "            'thik': u' th√≠ch ',u' sop ': u' c·ª≠a h√†ng ', ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': u' r·∫•t ',u'qu·∫£ ng ':u' qu·∫£ng  ',\n",
    "            'dep': u' ƒë·∫πp ',u' xau ': u' x·∫•u ','delicious': u' ngon ', u'h√†g': u' h√†ng ', u'q·ªßa': u' qu·∫£ ',\n",
    "            'iu': u' y√™u ','fake': u' gi·∫£ m·∫°o ', 'trl': 'tr·∫£ l·ªùi', '><': u' T√≠ch c·ª±c ', 'nv' : 'nh√¢n vi√™n', 'nvien' : 'nh√¢n vi√™n',\n",
    "            ' por ': u' t·ªá ',' poor ': u' t·ªá ', 'ib':u' nh·∫Øn tin ', 'rep':u' tr·∫£ l·ªùi ',u'fback':' feedback ','fedback':' feedback ', 'p√πn' : 'bu·ªìn', 'tu·ªµt v·ªùi' : 'tuy·ªát v·ªùi',\n",
    "            #d∆∞·ªõi 3* quy v·ªÅ 1*, tr√™n 3* quy v·ªÅ 5*\n",
    "            '6 sao': ' 5star ','6 star': ' 5star ', '5star': ' 5star ','5 sao': ' 5star ','5sao': ' 5star ',\n",
    "            'starstarstarstarstar': ' 5star ', '1 sao': ' 1star ', '1sao': ' 1star ','2 sao':' 1star ','2sao':' 1star ',\n",
    "            '2 starstar':' 1star ','1star': ' 1star ', '0 sao': ' 1star ', '0star': ' 1star ',\n",
    "            }\n",
    "\n",
    "    for k, v in replace_list.items():\n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "\n",
    "# T·ª´ ƒëi·ªÉn t√≠ch c·ª±c, ti√™u c·ª±c, ph·ªß ƒë·ªãnh\n",
    "def load_sentiment_dicts(path_pos, path_nag, path_not):\n",
    "    with codecs.open(path_pos, 'r', encoding='UTF-8') as f:\n",
    "        pos = f.readlines()\n",
    "    pos_list = [n.strip() for n in pos]\n",
    "\n",
    "    with codecs.open(path_nag, 'r', encoding='UTF-8') as f:\n",
    "        nag = f.readlines()\n",
    "    nag_list = [n.strip() for n in nag]\n",
    "\n",
    "    with codecs.open(path_not, 'r', encoding='UTF-8') as f:\n",
    "        not_ = f.readlines()\n",
    "    not_list = [n.strip() for n in not_]\n",
    "\n",
    "    return pos_list, nag_list, not_list\n",
    "\n",
    "# Ph√¢n t√≠ch t√¨nh c·∫£m b·∫±ng t·ª´ ƒëi·ªÉn ƒë∆∞·ª£c x√°c ƒë·ªãnh tr∆∞·ªõc\n",
    "def add_sentiment_features(text, pos_list, nag_list, not_list):\n",
    "    # chuyen punctuation th√†nh space\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    text = ViTokenizer.tokenize(text)\n",
    "    texts = text.split()\n",
    "    len_text = len(texts)\n",
    "\n",
    "    texts = [t.replace('_', ' ') for t in texts]\n",
    "    for i in range(len_text):\n",
    "        cp_text = texts[i]\n",
    "        if cp_text in not_list: # X·ª≠ l√Ω v·∫•n ƒë·ªÅ ph·ªß ƒë·ªãnh (VD: √°o n√†y ch·∫≥ng ƒë·∫πp--> √°o n√†y notpos)\n",
    "            numb_word = 2 if len_text - i - 1 >= 4 else len_text - i - 1\n",
    "\n",
    "            for j in range(numb_word):\n",
    "                if texts[i + j + 1] in pos_list:\n",
    "                    texts[i] = 'notpos'\n",
    "                    texts[i + j + 1] = ''\n",
    "\n",
    "                if texts[i + j + 1] in nag_list:\n",
    "                    texts[i] = 'notnag'\n",
    "                    texts[i + j + 1] = ''\n",
    "        else: #Th√™m feature cho nh·ªØng sentiment words (√°o n√†y ƒë·∫πp--> √°o n√†y ƒë·∫πp T√≠ch c·ª±c)\n",
    "            if cp_text in pos_list:\n",
    "                texts.append('T√≠ch c·ª±c')\n",
    "            elif cp_text in nag_list:\n",
    "                texts.append('Ti√™u c·ª±c')\n",
    "\n",
    "    text = u' '.join(texts)\n",
    "\n",
    "    #remove n·ªët nh·ªØng k√Ω t·ª± th·ª´a th√£i\n",
    "    text = text.replace(u'\"', u' ')\n",
    "    text = text.replace(u'Ô∏è', u'')\n",
    "    text = text.replace('üèª','')\n",
    "    return text\n",
    "\n",
    "# overall preprocessing\n",
    "def text_preprocess(document, pos_list, nag_list, not_list):\n",
    "    #ƒë∆∞a v·ªÅ lower\n",
    "    document = document.lower()\n",
    "    # x√≥a html code\n",
    "    document = remove_html(document)\n",
    "    # chu·∫©n h√≥a unicode\n",
    "    document = convert_unicode(document)\n",
    "    \n",
    "    # chu·∫©n h√≥a c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát\n",
    "    document = normalize_money(document)\n",
    "    document = normalize_hastag(document)\n",
    "    document = normalize_website(document)\n",
    "    document = nomalize_emoji(document)\n",
    "    document = normalize_elongate(document)\n",
    "    document = normalize_acronyms(document)\n",
    "    document = remove_numbers(document)\n",
    "\n",
    "    # chu·∫©n h√≥a c√°ch g√µ d·∫•u ti·∫øng vi·ªát\n",
    "    document = standardize_sentence_typing(document)\n",
    "    # t√°ch t·ª´\n",
    "    document = word_tokenize(document, format=\"text\")\n",
    "    # ƒë∆∞a v·ªÅ lower\n",
    "    document = document.lower()\n",
    "    # x√≥a c√°c k√Ω t·ª± kh√¥ng c·∫ßn thi·∫øt\n",
    "    document = remove_unnecessary(document)\n",
    "    #\n",
    "    document = add_sentiment_features(document, pos_list, nag_list, not_list)\n",
    "    return document.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation))).replace(' '*4, ' ').replace(' '*3, ' ').replace(' '*2, ' ').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pos = '../sentiment_dicts/not.txt'\n",
    "path_nag = '../sentiment_dicts/nag.txt'\n",
    "path_not = '../sentiment_dicts/not.txt'\n",
    "pos_list, nag_list, not_list = load_sentiment_dicts(path_pos, path_nag, path_not)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer.py\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from underthesea import word_tokenize\n",
    "def tokenize_all_reviews(embed_model, dims, reviews, pos_list, nag_list, not_list):\n",
    "    # split each review into a list of words\n",
    "    # reviews = [text_preprocess(review, pos_list, nag_list, not_list) for review in reviews]\n",
    "    # reviews_words = [ word_tokenize(review, format='text') for review in reviews]\n",
    "    reviews_words = [text_preprocess(review, pos_list, nag_list, not_list) for review in reviews]\n",
    "    # reviews_words = [word_tokenize(review) for review in reviews_words]\n",
    "    reviews_words = [reviews_word.split() for reviews_word in reviews_words]\n",
    "\n",
    "    tokenized_reviews = []\n",
    "    for review in reviews_words:\n",
    "        mean_embedding = np.zeros(dims)\n",
    "        num_words = 0\n",
    "        for word in review:\n",
    "            try:\n",
    "                mean_embedding += embed_model[word]\n",
    "                num_words += 1\n",
    "            except:\n",
    "                continue\n",
    "        mean_embedding /= num_words\n",
    "        tokenized_reviews.append(mean_embedding)\n",
    "\n",
    "    return tokenized_reviews \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load('/Mobile-e-commerce-review-sentiment-classification-main/w2v/w2v_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Link</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>TGDD</td>\n",
       "      <td>['M√°y n√≥ng r·∫•t t·ªá ,call cideo nh∆∞ mu·ªën n·ªï,ƒëang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>TGDD</td>\n",
       "      <td>['T·ªët', 'M√°y m·ªõi mua s√†i mau n√≥ng m√°y qu√°.kh√¥n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>TGDD</td>\n",
       "      <td>['R·∫•t tuy·ªát v·ªùi', 'Ok', 'R·∫•t T·ªët', 'M·ªçi th·ª© r·∫•...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>FPT</td>\n",
       "      <td>['M√¨nh m·ªõi mua tr·∫£ g√≥p. th·ªß t·ª•c nhanh m√† nh√¢n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iPhone 15 Pro Max</td>\n",
       "      <td>FPT</td>\n",
       "      <td>['Th·ªß t·ª•c mua h√†ng nhanh, ƒë∆°n gi·∫£n nh√©', 'S·ª≠ d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>iPhone 14 Plus</td>\n",
       "      <td>FPT</td>\n",
       "      <td>['m√†u xanh ƒë·∫πp l·∫Øm lu√¥n, c·∫ßm ch·∫Øc tay, m√†n h√¨n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>iPhone 14 Plus</td>\n",
       "      <td>FPT</td>\n",
       "      <td>['m√°y d√πng kh√° t·ªët, kh√¥ng c√≥ g√¨ ƒë·ªÉ ch√™, pin c≈©...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Samsung Galaxy A34 5G</td>\n",
       "      <td>TGDD</td>\n",
       "      <td>['D√πng t·∫°m ƒë∆∞·ª£c.nh∆∞ng khi mua th√¨ k ƒëc k√®m s·∫°c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Samsung Galaxy A34 5G</td>\n",
       "      <td>TGDD</td>\n",
       "      <td>['M√¨nh s√†i g·∫ßn nƒÉm r kh sao m√† g·∫ßn tu·∫ßn nay c·ª©...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Samsung Galaxy A34 5G</td>\n",
       "      <td>FPT</td>\n",
       "      <td>['Ko c√≥ g√¨ ƒë·ªÉ ch√™, nh∆∞ng ph·∫£i tr·∫£i nghi·ªám m·ªôt ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Name  Link  \\\n",
       "0        iPhone 15 Pro Max  TGDD   \n",
       "1        iPhone 15 Pro Max  TGDD   \n",
       "2        iPhone 15 Pro Max  TGDD   \n",
       "3        iPhone 15 Pro Max   FPT   \n",
       "4        iPhone 15 Pro Max   FPT   \n",
       "..                     ...   ...   \n",
       "142         iPhone 14 Plus   FPT   \n",
       "143         iPhone 14 Plus   FPT   \n",
       "144  Samsung Galaxy A34 5G  TGDD   \n",
       "145  Samsung Galaxy A34 5G  TGDD   \n",
       "146  Samsung Galaxy A34 5G   FPT   \n",
       "\n",
       "                                              Comments  \n",
       "0    ['M√°y n√≥ng r·∫•t t·ªá ,call cideo nh∆∞ mu·ªën n·ªï,ƒëang...  \n",
       "1    ['T·ªët', 'M√°y m·ªõi mua s√†i mau n√≥ng m√°y qu√°.kh√¥n...  \n",
       "2    ['R·∫•t tuy·ªát v·ªùi', 'Ok', 'R·∫•t T·ªët', 'M·ªçi th·ª© r·∫•...  \n",
       "3    ['M√¨nh m·ªõi mua tr·∫£ g√≥p. th·ªß t·ª•c nhanh m√† nh√¢n ...  \n",
       "4    ['Th·ªß t·ª•c mua h√†ng nhanh, ƒë∆°n gi·∫£n nh√©', 'S·ª≠ d...  \n",
       "..                                                 ...  \n",
       "142  ['m√†u xanh ƒë·∫πp l·∫Øm lu√¥n, c·∫ßm ch·∫Øc tay, m√†n h√¨n...  \n",
       "143  ['m√°y d√πng kh√° t·ªët, kh√¥ng c√≥ g√¨ ƒë·ªÉ ch√™, pin c≈©...  \n",
       "144  ['D√πng t·∫°m ƒë∆∞·ª£c.nh∆∞ng khi mua th√¨ k ƒëc k√®m s·∫°c...  \n",
       "145  ['M√¨nh s√†i g·∫ßn nƒÉm r kh sao m√† g·∫ßn tu·∫ßn nay c·ª©...  \n",
       "146  ['Ko c√≥ g√¨ ƒë·ªÉ ch√™, nh∆∞ng ph·∫£i tr·∫£i nghi·ªám m·ªôt ...  \n",
       "\n",
       "[147 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(df, name, company):\n",
    "    comments = df.loc[(df['Name'] == name) & (df['Link'] == company), 'Comments'].values\n",
    "    return comments\n",
    "def list_comments(l_comments):\n",
    "    all_comments = []\n",
    "    for i in range(len(l_comments)):\n",
    "        split_comments = ast.literal_eval(l_comments[i])\n",
    "        for comment in split_comments:\n",
    "            all_comments.append(comment)\n",
    "    return all_comments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name = get_comments(pre_df1, 'Samsung Galaxy A34 5G', 'FPT')\n",
    "product_name = list_comments(product_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_word = tokenize_all_reviews(w2v, 300, product_name, pos_list, nag_list, not_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_word_np = np.array(vec_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 300)"
      ]
     },
     "execution_count": 808,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_word_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_mask = np.isnan(vec_word_np)\n",
    "\n",
    "rows_with_nan = nan_mask.any(axis=1)\n",
    "\n",
    "cleaned_array = vec_word_np[~rows_with_nan]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(342, 300)"
      ]
     },
     "execution_count": 799,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_word = cleaned_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "345"
      ]
     },
     "execution_count": 798,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(product_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trnmah/mambaforge/envs/practic1/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.2.2 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "# svm_Pin = joblib.load('/home/trnmah/CNPM66/Mobile-e-commerce-review-sentiment-classification-main/pretrained_svm/bestModelPin')\n",
    "# svm_Service = joblib.load('/home/trnmah/CNPM66/Mobile-e-commerce-review-sentiment-classification-main/pretrained_svm/bestModelService')\n",
    "# svm_General = joblib.load('/home/trnmah/CNPM66/Mobile-e-commerce-review-sentiment-classification-main/pretrained_svm/bestModelGeneral')\n",
    "# svm_Others = joblib.load('/home/trnmah/CNPM66/Mobile-e-commerce-review-sentiment-classification-main/pretrained_svm/bestModelOthers')\n",
    "\n",
    "svm_SPin = joblib.load('/home/trnmah/CNPM66/Mobile-e-commerce-review-sentiment-classification-main/pretrained_svm/bestModelSPin')\n",
    "svm_SSer = joblib.load('/home/trnmah/CNPM66/Mobile-e-commerce-review-sentiment-classification-main/pretrained_svm/bestModelSSer')\n",
    "svm_SGeneral = joblib.load('/home/trnmah/CNPM66/Mobile-e-commerce-review-sentiment-classification-main/pretrained_svm/bestModelSGeneral')\n",
    "svm_SOth = joblib.load('/home/trnmah/CNPM66/Mobile-e-commerce-review-sentiment-classification-main/pretrained_svm/bestModelSOth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_Pin = svm_Pin.predict(vec_word_np)\n",
    "# y_pred_Service = svm_Service.predict(vec_word_np)\n",
    "# y_pred_General = svm_General.predict(vec_word_np)\n",
    "# y_pred_Others = svm_Others.predict(vec_word_np)\n",
    "\n",
    "# y_pred_SPin = svm_SPin.predict_proba(vec_word)\n",
    "# y_pred_SSer = svm_SSer.predict_proba(vec_word)\n",
    "# y_pred_SGeneral = svm_SGeneral.predict_proba(vec_word)\n",
    "# y_pred_SOth = svm_SOth.predict_proba(vec_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_SPin = svm_SPin.predict_proba(vec_word)\n",
    "y_pred_SSer = svm_SSer.predict_proba(vec_word)\n",
    "y_pred_SGeneral = svm_SGeneral.predict_proba(vec_word)\n",
    "y_pred_SOth = svm_SOth.predict_proba(vec_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04756708 0.7865609  0.16587201]\n",
      "[0.0428561  0.50355833 0.45358557]\n",
      "[0.04055891 0.54013136 0.41930973]\n",
      "[0.07349256 0.30205295 0.62445449]\n"
     ]
    }
   ],
   "source": [
    "# print(y_pred_Pin.mean(axis=0))\n",
    "# print(y_pred_Service.mean(axis=0))\n",
    "# print(y_pred_General.mean(axis=0))\n",
    "# print(y_pred_Others.mean(axis=0))\n",
    "print(y_pred_SPin.mean(axis=0))\n",
    "print(y_pred_SSer.mean(axis=0))\n",
    "print(y_pred_SGeneral.mean(axis=0))\n",
    "print(y_pred_SOth.mean(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practic1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
